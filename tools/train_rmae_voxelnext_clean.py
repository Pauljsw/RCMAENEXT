#!/usr/bin/env python
"""
tools/train_rmae_voxelnext_clean.py

ğŸ¯ Clean R-MAE + VoxelNeXt Training Script
ê³µì‹ R-MAE GitHub ì½”ë“œì˜ train_ssl.py ìŠ¤íƒ€ì¼ë¡œ ì¬êµ¬ì„±

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ê³µì‹ R-MAE ì½”ë“œì˜ ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì ì¸ í›ˆë ¨ ë¡œì§ ì°¨ìš©
2. ë³µì¡í•œ ëª¨ë‹ˆí„°ë§ ì œê±°í•˜ê³  í•µì‹¬ ê¸°ëŠ¥ë§Œ ìœ ì§€
3. ì•ˆì •ì ì¸ pretraining/fine-tuning ì§€ì›
4. ê¸°ì¡´ train_voxel_mae.pyì™€ í˜¸í™˜ì„± ìœ ì§€

Usage:
    # Pretraining
    python train_rmae_voxelnext_clean.py \
        --cfg_file cfgs/custom_models/rmae_voxelnext_clean_pretraining.yaml \
        --batch_size 4 \
        --epochs 30 \
        --extra_tag rmae_clean_pretraining
    
    # Fine-tuning  
    python train_rmae_voxelnext_clean.py \
        --cfg_file cfgs/custom_models/rmae_voxelnext_clean_finetune.yaml \
        --pretrained_model output/.../checkpoint_epoch_30.pth \
        --batch_size 4 \
        --epochs 50 \
        --extra_tag rmae_clean_finetune
"""

import _init_path
import argparse
import datetime
import glob
import os
from pathlib import Path
import numpy as np
import tqdm

import torch
import torch.nn as nn
from tensorboardX import SummaryWriter

from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file
from pcdet.datasets import build_dataloader
from pcdet.models import build_network
from pcdet.utils import common_utils
from train_utils.optimization import build_optimizer, build_scheduler
from train_utils.train_utils import save_checkpoint, checkpoint_state


def parse_config():
    """ğŸ¯ ê³µì‹ R-MAE ìŠ¤íƒ€ì¼ì˜ argument parsing"""
    parser = argparse.ArgumentParser(description='Clean R-MAE VoxelNeXt Training')
    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')
    parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')
    parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')
    parser.add_argument('--workers', type=int, default=4, help='number of workers for dataloader')
    parser.add_argument('--extra_tag', type=str, default='clean_rmae', help='extra tag for this experiment')
    parser.add_argument('--ckpt', type=str, default=None, help='checkpoint to start from')
    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained model for fine-tuning')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')
    parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distributed training')
    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')
    parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')
    parser.add_argument('--fix_random_seed', action='store_true', default=False, help='fix random seed')
    parser.add_argument('--ckpt_save_interval', type=int, default=1, help='number of training epochs')
    parser.add_argument('--max_ckpt_save_num', type=int, default=30, help='max number of saved checkpoint')
    parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')
    parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER, help='set extra config keys if needed')

    args = parser.parse_args()

    cfg_from_yaml_file(args.cfg_file, cfg)
    cfg.TAG = Path(args.cfg_file).stem
    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1:-1])

    if args.set_cfgs is not None:
        cfg_from_list(args.set_cfgs, cfg)

    return args, cfg


def model_fn_decorator():
    """ğŸ¯ ê³µì‹ R-MAE ìŠ¤íƒ€ì¼ì˜ model function decorator"""
    def model_func(model, batch_dict):
        load_data_to_gpu(batch_dict)
        ret_dict, tb_dict, disp_dict = model(batch_dict)

        loss = ret_dict['loss'].mean()
        
        # Global step ì—…ë°ì´íŠ¸
        if hasattr(model, 'update_global_step'):
            model.update_global_step()
        else:
            model.module.update_global_step()

        return loss, tb_dict, disp_dict

    return model_func


def load_data_to_gpu(batch_dict):
    """ğŸ¯ ê³µì‹ ìŠ¤íƒ€ì¼ì˜ GPU ë°ì´í„° ë¡œë”©"""
    for key, val in batch_dict.items():
        if not isinstance(val, np.ndarray):
            continue
        elif key in ['frame_id', 'metadata', 'calib']:
            continue
        elif key in ['image_shape']:
            batch_dict[key] = torch.from_numpy(val).int().cuda()
        else:
            batch_dict[key] = torch.from_numpy(val).float().cuda()


def train_one_epoch_clean(model, optimizer, train_loader, model_func, lr_scheduler, accumulated_iter, optim_cfg,
                         rank, tbar, tb_log=None, leave_pbar=False, total_it_each_epoch=None, dataloader_iter=None):
    """ğŸ¯ ê³µì‹ R-MAE ìŠ¤íƒ€ì¼ì˜ clean epoch training"""
    if total_it_each_epoch == len(train_loader):
        dataloader_iter = iter(train_loader)

    if rank == 0:
        pbar = tqdm.tqdm(total=total_it_each_epoch, leave=leave_pbar, desc='train', dynamic_ncols=True)

    for cur_it in range(total_it_each_epoch):
        try:
            batch = next(dataloader_iter)
        except StopIteration:
            dataloader_iter = iter(train_loader)
            batch = next(dataloader_iter)

        lr_scheduler.step(accumulated_iter)

        try:
            cur_lr = float(optimizer.lr)
        except:
            cur_lr = optimizer.param_groups[0]['lr']

        if tb_log is not None:
            tb_log.add_scalar('meta_data/learning_rate', cur_lr, accumulated_iter)

        model.train()
        optimizer.zero_grad()

        # ğŸ“ Forward pass
        loss, tb_dict, disp_dict = model_func(model, batch)

        # ğŸ“ Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), optim_cfg.GRAD_NORM_CLIP)
        optimizer.step()

        accumulated_iter += 1
        disp_dict.update({'loss': loss.item(), 'lr': cur_lr})

        # ğŸ“ Logging (ê³µì‹ ìŠ¤íƒ€ì¼ë¡œ ë‹¨ìˆœí™”)
        if rank == 0:
            pbar.update()
            pbar.set_postfix(dict(total_it=accumulated_iter))
            tbar.set_postfix(disp_dict)
            tbar.refresh()

            if tb_log is not None:
                tb_log.add_scalar('train/loss', loss, accumulated_iter)
                tb_log.add_scalar('meta_data/learning_rate', cur_lr, accumulated_iter)
                for key, val in tb_dict.items():
                    tb_log.add_scalar('train/' + key, val, accumulated_iter)
    
    if rank == 0:
        pbar.close()
    return accumulated_iter


def train_model_clean(model, optimizer, train_loader, model_func, lr_scheduler, optim_cfg,
                     start_epoch, total_epochs, start_iter, rank, tb_log, ckpt_save_dir,
                     train_sampler=None, lr_warmup_scheduler=None, ckpt_save_interval=1,
                     max_ckpt_save_num=50, merge_all_iters_to_one_epoch=False):
    """ğŸ¯ ê³µì‹ R-MAE ìŠ¤íƒ€ì¼ì˜ clean model training"""
    
    accumulated_iter = start_iter
    dataloader_iter = iter(train_loader)
    
    # ğŸ“ Pretraining/Fine-tuning ëª¨ë“œ ì²´í¬
    is_pretraining = (hasattr(cfg.MODEL.BACKBONE_3D, 'PRETRAINING') and 
                     cfg.MODEL.BACKBONE_3D.PRETRAINING)
    mode_name = "Pretraining" if is_pretraining else "Fine-tuning"
    
    print(f"ğŸ¯ Clean R-MAE {mode_name} Started")
    
    with tqdm.trange(start_epoch, total_epochs, desc='epochs', dynamic_ncols=True, leave=(rank == 0)) as tbar:
        total_it_each_epoch = len(train_loader)
        
        for cur_epoch in tbar:
            if train_sampler is not None:
                train_sampler.set_epoch(cur_epoch)

            # ğŸ“ Scheduler ì„ íƒ
            if lr_warmup_scheduler is not None and cur_epoch < optim_cfg.WARMUP_EPOCH:
                cur_scheduler = lr_warmup_scheduler
            else:
                cur_scheduler = lr_scheduler

            # ğŸ“ í•œ epoch í›ˆë ¨
            accumulated_iter = train_one_epoch_clean(
                model, optimizer, train_loader, model_func,
                lr_scheduler=cur_scheduler,
                accumulated_iter=accumulated_iter, optim_cfg=optim_cfg, rank=rank, tbar=tbar,
                tb_log=tb_log, leave_pbar=(cur_epoch + 1 == total_epochs),
                total_it_each_epoch=total_it_each_epoch, dataloader_iter=dataloader_iter
            )

            # ğŸ“ Checkpoint ì €ì¥
            trained_epoch = cur_epoch + 1
            if trained_epoch % ckpt_save_interval == 0 and rank == 0:

                ckpt_list = glob.glob(str(ckpt_save_dir / 'checkpoint_epoch_*.pth'))
                ckpt_list.sort(key=os.path.getmtime)

                if ckpt_list.__len__() >= max_ckpt_save_num:
                    for cur_file_idx in range(0, len(ckpt_list) - max_ckpt_save_num + 1):
                        os.remove(ckpt_list[cur_file_idx])

                ckpt_name = ckpt_save_dir / ('checkpoint_epoch_%d' % trained_epoch)
                save_checkpoint(
                    checkpoint_state(model, optimizer, trained_epoch, accumulated_iter), filename=ckpt_name,
                )


def main():
    """ğŸ¯ Clean R-MAE VoxelNeXt ë©”ì¸ í•¨ìˆ˜"""
    args, cfg = parse_config()
    
    # ğŸ“ ë¶„ì‚° í›ˆë ¨ ì„¤ì •
    if args.launcher == 'none':
        dist_train = False
        total_gpus = 1
    else:
        total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(
            args.tcp_port, args.local_rank, backend='nccl'
        )
        dist_train = True

    if args.batch_size is None:
        args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU
    else:
        assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'
        args.batch_size = args.batch_size // total_gpus

    args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs

    if args.fix_random_seed:
        common_utils.set_random_seed(666 + cfg.LOCAL_RANK)

    # ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = cfg.ROOT_DIR / 'output' / cfg.EXP_GROUP_PATH / cfg.TAG / args.extra_tag
    ckpt_dir = output_dir / 'ckpt'
    output_dir.mkdir(parents=True, exist_ok=True)
    ckpt_dir.mkdir(parents=True, exist_ok=True)

    log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
    logger = common_utils.create_logger(log_file, rank=cfg.LOCAL_RANK)

    # ğŸ“ ë¡œê·¸ ì„¤ì •
    logger.info('**********************Start logging**********************')
    gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'
    logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)

    if dist_train:
        logger.info('total_batch_size: %d' % (total_gpus * args.batch_size))
    for key, val in vars(args).items():
        logger.info('{:16} {}'.format(key, val))
    log_config_to_file(cfg, logger=logger)
    if cfg.LOCAL_RANK == 0:
        os.system('cp %s %s' % (args.cfg_file, output_dir))

    tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None

    # ğŸ“ ë°ì´í„°ë¡œë” & ë„¤íŠ¸ì›Œí¬ & ì˜µí‹°ë§ˆì´ì € ìƒì„±
    train_set, train_loader, train_sampler = build_dataloader(
        dataset_cfg=cfg.DATA_CONFIG,
        class_names=cfg.CLASS_NAMES,
        batch_size=args.batch_size,
        dist=dist_train, workers=args.workers,
        logger=logger,
        training=True,
        merge_all_iters_to_one_epoch=False,
        total_epochs=args.epochs
    )

    model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=train_set)
    
    if args.sync_bn:
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
    model.cuda()

    optimizer = build_optimizer(model, cfg.OPTIMIZATION)

    start_epoch = it = 0
    last_epoch = -1
    
    # ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
    if args.pretrained_model is not None:
        model.load_params_from_file(filename=args.pretrained_model, to_cpu=dist_train, logger=logger)
    
    if args.ckpt is not None:
        it, start_epoch = model.load_params_with_optimizer(args.ckpt, to_cpu=dist_train, optimizer=optimizer, logger=logger)
        last_epoch = start_epoch + 1

    model.train()
    if dist_train:
        model = nn.parallel.DistributedDataParallel(model, device_ids=[cfg.LOCAL_RANK % torch.cuda.device_count()])
    logger.info(model)

    lr_scheduler, lr_warmup_scheduler = build_scheduler(
        optimizer, total_iters_each_epoch=len(train_loader), total_epochs=args.epochs,
        last_epoch=last_epoch, optim_cfg=cfg.OPTIMIZATION
    )

    # ğŸ“ í›ˆë ¨ ì‹œì‘
    mode_name = "Pretraining" if getattr(cfg.MODEL.BACKBONE_3D, 'PRETRAINING', False) else "Fine-tuning"
    logger.info('**********************Start Clean R-MAE %s %s/%s(%s)**********************'
                % (mode_name, cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))
    
    train_model_clean(
        model,
        optimizer,
        train_loader,
        model_func=model_fn_decorator(),
        lr_scheduler=lr_scheduler,
        optim_cfg=cfg.OPTIMIZATION,
        start_epoch=start_epoch,
        total_epochs=args.epochs,
        start_iter=it,
        rank=cfg.LOCAL_RANK,
        tb_log=tb_log,
        ckpt_save_dir=ckpt_dir,
        train_sampler=train_sampler,
        lr_warmup_scheduler=lr_warmup_scheduler,
        ckpt_save_interval=args.ckpt_save_interval,
        max_ckpt_save_num=args.max_ckpt_save_num,
        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch
    )

    logger.info('**********************Clean R-MAE %s is finished**********************' % mode_name)


if __name__ == '__main__':
    main()