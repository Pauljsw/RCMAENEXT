import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional

class SimpleRealisticIntensityGenerator:
    """
    Real dataì˜ í†µê³„ì  íŠ¹ì„±ì„ íŒŒì•…í•´ì„œ 
    Synthetic dataì— í•©ë¦¬ì ì¸ intensityë¥¼ ìƒì„±
    """
    
    def __init__(self, real_data_path: str = None):
        self.real_data_path = Path(real_data_path) if real_data_path else None
        self.real_stats = None
        
    def analyze_real_data_statistics(self, max_files: int = 20):
        """Real dataì˜ ê¸°ë³¸ í†µê³„ë§Œ íŒŒì•…"""
        if not self.real_data_path or not self.real_data_path.exists():
            print("âš ï¸ Real data path not found, using default statistics")
            return self._get_default_stats()
        
        print(f"ğŸ“Š Analyzing real data statistics from {self.real_data_path}")
        
        # Real files ì°¾ê¸°
        real_files = (list(self.real_data_path.glob("*.bin")) + 
                     list(self.real_data_path.glob("*.pcd")) + 
                     list(self.real_data_path.glob("*.npy")))
        
        if not real_files:
            print("âš ï¸ No real data files found, using default statistics")
            return self._get_default_stats()
        
        print(f"Found {len(real_files)} files, analyzing {min(max_files, len(real_files))} files")
        
        all_intensities = []
        all_distances = []
        all_heights = []
        
        for i, file_path in enumerate(real_files[:max_files]):
            try:
                # íŒŒì¼ ë¡œë“œ
                if file_path.suffix == '.npy':
                    points = np.load(file_path)
                elif file_path.suffix == '.bin':
                    points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)
                else:
                    continue
                
                if points.shape[1] < 4:
                    continue
                
                # ê¸°ë³¸ í•„í„°ë§
                distances = np.linalg.norm(points[:, :3], axis=1)
                intensities = points[:, 3]
                heights = points[:, 2]
                
                valid_mask = (distances > 1.0) & (distances < 80.0) & (intensities > 0)
                
                all_intensities.extend(intensities[valid_mask])
                all_distances.extend(distances[valid_mask])
                all_heights.extend(heights[valid_mask])
                
                if (i + 1) % 5 == 0:
                    print(f"  Processed {i+1}/{min(max_files, len(real_files))} files")
                    
            except Exception as e:
                print(f"  Error loading {file_path.name}: {e}")
        
        if not all_intensities:
            print("âŒ Failed to load any valid data, using default statistics")
            return self._get_default_stats()
        
        # í†µê³„ ê³„ì‚°
        all_intensities = np.array(all_intensities)
        all_distances = np.array(all_distances)
        all_heights = np.array(all_heights)
        
        # ê¸°ë³¸ í†µê³„
        intensity_stats = {
            'mean': np.mean(all_intensities),
            'std': np.std(all_intensities),
            'min': np.min(all_intensities),
            'max': np.max(all_intensities),
            'median': np.median(all_intensities),
            'p25': np.percentile(all_intensities, 25),
            'p75': np.percentile(all_intensities, 75)
        }
        
        # ê±°ë¦¬ë³„ intensity (ëŒ€ëµì )
        distance_groups = []
        for dist_range in [(1, 10), (10, 20), (20, 40), (40, 80)]:
            mask = (all_distances >= dist_range[0]) & (all_distances < dist_range[1])
            if np.sum(mask) > 100:
                group_intensities = all_intensities[mask]
                distance_groups.append({
                    'distance_range': dist_range,
                    'mean_intensity': np.mean(group_intensities),
                    'std_intensity': np.std(group_intensities),
                    'count': np.sum(mask)
                })
        
        # ë†’ì´ë³„ intensity (ëŒ€ëµì )
        height_groups = []
        for height_range in [(0, 1), (1, 2), (2, 4), (4, 10)]:
            mask = (all_heights >= height_range[0]) & (all_heights < height_range[1])
            if np.sum(mask) > 100:
                group_intensities = all_intensities[mask]
                height_groups.append({
                    'height_range': height_range,
                    'mean_intensity': np.mean(group_intensities),
                    'std_intensity': np.std(group_intensities),
                    'count': np.sum(mask)
                })
        
        self.real_stats = {
            'total_points': len(all_intensities),
            'intensity': intensity_stats,
            'distance_groups': distance_groups,
            'height_groups': height_groups,
            'distance_range': (np.min(all_distances), np.max(all_distances)),
            'height_range': (np.min(all_heights), np.max(all_heights))
        }
        
        print(f"âœ… Real data analysis completed:")
        print(f"   ğŸ“Š Intensity: mean={intensity_stats['mean']:.1f}, std={intensity_stats['std']:.1f}")
        print(f"   ğŸ“ Distance groups: {len(distance_groups)}")
        print(f"   ğŸ“ Height groups: {len(height_groups)}")
        
        return self.real_stats
    
    def _get_default_stats(self):
        """Real dataê°€ ì—†ì„ ë•Œ ê¸°ë³¸ í†µê³„"""
        return {
            'intensity': {
                'mean': 35.0, 'std': 25.0, 'min': 1.0, 'max': 200.0,
                'median': 30.0, 'p25': 15.0, 'p75': 50.0
            },
            'distance_groups': [
                {'distance_range': (1, 10), 'mean_intensity': 45.0, 'std_intensity': 20.0},
                {'distance_range': (10, 20), 'mean_intensity': 35.0, 'std_intensity': 18.0},
                {'distance_range': (20, 40), 'mean_intensity': 28.0, 'std_intensity': 15.0},
                {'distance_range': (40, 80), 'mean_intensity': 22.0, 'std_intensity': 12.0}
            ],
            'height_groups': [
                {'height_range': (0, 1), 'mean_intensity': 25.0, 'std_intensity': 15.0},    # ë°”í€´/í•˜ë¶€
                {'height_range': (1, 2), 'mean_intensity': 40.0, 'std_intensity': 20.0},   # ì°¨ì²´
                {'height_range': (2, 4), 'mean_intensity': 35.0, 'std_intensity': 18.0},   # ìƒë¶€
                {'height_range': (4, 10), 'mean_intensity': 30.0, 'std_intensity': 16.0}   # ìºë¹ˆ ë“±
            ]
        }
    
    def generate_synthetic_intensity(self, synthetic_points: np.ndarray) -> np.ndarray:
        """Real data íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ synthetic intensity ìƒì„±"""
        
        if self.real_stats is None:
            print("âš ï¸ No real data stats available, using defaults")
            self.real_stats = self._get_default_stats()
        
        n_points = synthetic_points.shape[0]
        distances = np.linalg.norm(synthetic_points, axis=1)
        heights = synthetic_points[:, 2]
        
        print(f"ğŸ¯ Generating intensity for {n_points} points based on real data patterns")
        
        # 1. ê±°ë¦¬ë³„ ê¸°ë³¸ intensity
        base_intensities = np.zeros(n_points)
        
        for group in self.real_stats['distance_groups']:
            dist_min, dist_max = group['distance_range']
            mask = (distances >= dist_min) & (distances < dist_max)
            
            if np.sum(mask) > 0:
                # í•´ë‹¹ ê±°ë¦¬ ë²”ìœ„ì˜ í‰ê· ê°’ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                group_mean = group['mean_intensity']
                group_std = group.get('std_intensity', 15.0)
                
                # ì•½ê°„ì˜ ëœë¤ì„± ì¶”ê°€
                base_intensities[mask] = np.random.normal(
                    group_mean, group_std * 0.3, np.sum(mask)
                )
        
        # ê±°ë¦¬ ê·¸ë£¹ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì ë“¤ ì²˜ë¦¬
        unassigned_mask = base_intensities == 0
        if np.sum(unassigned_mask) > 0:
            # ì „ì²´ í‰ê· ê°’ ì‚¬ìš©
            overall_mean = self.real_stats['intensity']['mean']
            overall_std = self.real_stats['intensity']['std']
            base_intensities[unassigned_mask] = np.random.normal(
                overall_mean, overall_std * 0.5, np.sum(unassigned_mask)
            )
        
        print(f"   Distance-based intensity: {base_intensities.min():.1f} - {base_intensities.max():.1f}")
        
        # 2. ë†’ì´ë³„ ì¡°ì •
        height_adjusted = base_intensities.copy()
        
        for group in self.real_stats['height_groups']:
            height_min, height_max = group['height_range']
            mask = (heights >= height_min) & (heights < height_max)
            
            if np.sum(mask) > 0:
                # ë†’ì´ë³„ íŠ¹ì„±ì„ ë¹„ìœ¨ë¡œ ì ìš©
                current_mean = np.mean(base_intensities[mask])
                target_mean = group['mean_intensity']
                
                if current_mean > 0:
                    adjustment_factor = target_mean / current_mean
                    adjustment_factor = np.clip(adjustment_factor, 0.7, 1.4)  # ê·¹ë‹¨ì  ë³€í™” ë°©ì§€
                    height_adjusted[mask] *= adjustment_factor
        
        print(f"   Height-adjusted intensity: {height_adjusted.min():.1f} - {height_adjusted.max():.1f}")
        
        # 3. ì•½ê°„ì˜ ê³µê°„ì  ë³€í™” (í˜„ì‹¤ì )
        x, y = synthetic_points[:, 0], synthetic_points[:, 1]
        angles = np.arctan2(y, x)
        
        # ë°©í–¥ë³„ ì•½ê°„ì˜ ë³€í™” (ì„¼ì„œ íŠ¹ì„± ëª¨ì‚¬)
        angular_factor = 1.0 + 0.15 * np.sin(angles * 2)  # Â±15% ë³€í™”
        spatial_adjusted = height_adjusted * angular_factor
        
        print(f"   Spatially-adjusted intensity: {spatial_adjusted.min():.1f} - {spatial_adjusted.max():.1f}")
        
        # 4. Real dataì™€ ìœ ì‚¬í•œ ë…¸ì´ì¦ˆ íŠ¹ì„±
        real_std_ratio = self.real_stats['intensity']['std'] / self.real_stats['intensity']['mean']
        noise_std = spatial_adjusted * real_std_ratio * 0.3  # 30% ì ìš©
        noise_std = np.clip(noise_std, 1.0, 10.0)  # í•©ë¦¬ì  ë²”ìœ„
        
        noise = np.random.normal(0, noise_std)
        final_intensities = spatial_adjusted + noise
        
        # 5. Real data ë²”ìœ„ì— ë§ì¶° ì¡°ì •
        real_min = max(1.0, self.real_stats['intensity']['min'])
        real_max = min(255.0, self.real_stats['intensity']['max'])
        
        final_clipped = np.clip(final_intensities, real_min, real_max)
        
        print(f"   Final intensity: {final_clipped.min():.1f} - {final_clipped.max():.1f}, mean: {final_clipped.mean():.1f}")
        
        # 6. Real dataì™€ í†µê³„ ë¹„êµ
        real_mean = self.real_stats['intensity']['mean']
        synthetic_mean = np.mean(final_clipped)
        print(f"   ğŸ“Š Real vs Synthetic: {real_mean:.1f} vs {synthetic_mean:.1f} (diff: {abs(real_mean-synthetic_mean):.1f})")
        
        return final_clipped.astype(np.uint8)
    
    def save_stats(self, output_path: str):
        """ë¶„ì„ëœ í†µê³„ ì €ì¥"""
        if self.real_stats:
            with open(output_path, 'wb') as f:
                pickle.dump(self.real_stats, f)
            print(f"ğŸ’¾ Real data statistics saved to {output_path}")
    
    def load_stats(self, input_path: str):
        """ì €ì¥ëœ í†µê³„ ë¡œë“œ"""
        with open(input_path, 'rb') as f:
            self.real_stats = pickle.load(f)
        print(f"ğŸ“‚ Real data statistics loaded from {input_path}")

# í†µí•© í”„ë¡œì„¸ì„œ
class IntensityProcessor:
    """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬"""
    
    def __init__(self, real_data_path: str):
        self.generator = SimpleRealisticIntensityGenerator(real_data_path)
    
    def process_synthetic_dataset(self, 
                                synthetic_data_path: str,
                                output_path: str):
        """ì „ì²´ synthetic dataset ì²˜ë¦¬"""
        
        print("ğŸš€ Starting Intensity Processing Pipeline")
        print("="*50)
        
        # 1. Real data ë¶„ì„
        print("ğŸ“Š STEP 1: Analyzing Real Data")
        stats = self.generator.analyze_real_data_statistics(max_files=1246)
        
        if not stats:
            print("âŒ Failed to analyze real data")
            return False
        
        # 2. Synthetic files ì°¾ê¸°
        print(f"\nğŸ”§ STEP 2: Processing Synthetic Data")
        synthetic_path = Path(synthetic_data_path)
        synthetic_files = (list(synthetic_path.glob("*.bin")) + 
                          list(synthetic_path.glob("*.npy")) + 
                          list(synthetic_path.glob("*.pcd")))
        
        if not synthetic_files:
            print(f"âŒ No synthetic files found in {synthetic_data_path}")
            return False
        
        print(f"Found {len(synthetic_files)} synthetic files")
        
        # 3. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(output_path)
        output_dir.mkdir(exist_ok=True)
        
        # 4. ê° íŒŒì¼ ì²˜ë¦¬
        success_count = 0
        for i, file_path in enumerate(synthetic_files):
            try:
                print(f"Processing ({i+1}/{len(synthetic_files)}): {file_path.name}")
                
                # íŒŒì¼ ë¡œë“œ
                if file_path.suffix == '.npy':
                    points = np.load(file_path)
                elif file_path.suffix == '.bin':
                    points = np.fromfile(file_path, dtype=np.float32)
                    if len(points) % 3 == 0:
                        points = points.reshape(-1, 3)
                    else:
                        print(f"  âš ï¸ Unexpected format: {file_path.name}")
                        continue
                else:
                    continue
                
                if points.shape[1] != 3:
                    print(f"  âš ï¸ Expected 3 columns, got {points.shape[1]}: {file_path.name}")
                    continue
                
                # Intensity ìƒì„±
                intensities = self.generator.generate_synthetic_intensity(points)
                
                # x,y,z,intensity ê²°í•©
                enhanced_points = np.column_stack([points, intensities])
                
                # NPYë¡œ ì €ì¥
                output_file = output_dir / f"{file_path.stem}.npy"
                np.save(output_file, enhanced_points.astype(np.float32))
                
                success_count += 1
                
            except Exception as e:
                print(f"  âŒ Error processing {file_path.name}: {e}")
        
        print(f"\nâœ… Processing completed!")
        print(f"   Successfully processed: {success_count}/{len(synthetic_files)} files")
        print(f"   Output directory: {output_path}")
        
        # í†µê³„ ì €ì¥
        self.generator.save_stats(output_dir / "real_data_stats.pkl")
        
        return success_count > 0

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ê°„ë‹¨í•œ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) != 4:
        print("Usage: python script.py <real_data_path> <synthetic_data_path> <output_path>")
        print("Example: python script.py /data/real /data/synthetic /data/enhanced")
        return
    
    real_data_path = sys.argv[1]
    synthetic_data_path = sys.argv[2] 
    output_path = sys.argv[3]
    
    processor = IntensityProcessor(real_data_path)
    success = processor.process_synthetic_dataset(synthetic_data_path, output_path)
    
    if success:
        print("ğŸ‰ All done! Enhanced synthetic data is ready for R-MAE training.")
    else:
        print("âŒ Processing failed!")

if __name__ == "__main__":
    main()