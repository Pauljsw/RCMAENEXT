"""
train_utils/differential_lr_optimizer.py

ğŸ¯ ì°¨ë“± í•™ìŠµë¥  Optimizer ëª¨ë“ˆ
ê¸°ì¡´ OpenPCDet í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì— ê°„ë‹¨íˆ í†µí•© ê°€ëŠ¥

ì‚¬ìš©ë²•:
from train_utils.differential_lr_optimizer import build_differential_optimizer

# ê¸°ì¡´ build_optimizer ëŒ€ì‹  ì‚¬ìš©
optimizer = build_differential_optimizer(model, cfg.OPTIMIZATION)
"""

import torch


def build_differential_optimizer(model, optim_cfg):
    """
    ğŸ¯ ì°¨ë“± í•™ìŠµë¥  Optimizer ë¹Œë”
    
    ê¸°ì¡´ OpenPCDetì˜ build_optimizerë¥¼ ëŒ€ì²´í•˜ì—¬ ì‚¬ìš©
    Configì—ì„œ ì°¨ë“± í•™ìŠµë¥  ì„¤ì •ì„ ì½ì–´ì™€ ìë™ ì ìš©
    """
    
    # ì°¨ë“± í•™ìŠµë¥  ì„¤ì • í™•ì¸
    if hasattr(optim_cfg, 'DIFFERENTIAL_LR') and optim_cfg.DIFFERENTIAL_LR.get('ENABLED', False):
        return _build_differential_lr_optimizer(model, optim_cfg)
    else:
        # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
        from train_utils.optimization import build_optimizer
        return build_optimizer(model, optim_cfg)


def _build_differential_lr_optimizer(model, optim_cfg):
    """ì°¨ë“± í•™ìŠµë¥  Optimizer êµ¬í˜„"""
    
    # ê¸°ë³¸ ì„¤ì •
    base_lr = optim_cfg.DIFFERENTIAL_LR.get('BASE_LR', optim_cfg.LR)
    lr_ratios = optim_cfg.DIFFERENTIAL_LR.get('LR_RATIOS', {
        'BACKBONE_EARLY': 0.1,
        'BACKBONE_LATE': 0.5, 
        'DETECTION_HEAD': 1.0,
        'OTHER': 0.3
    })
    weight_decay_cfg = optim_cfg.DIFFERENTIAL_LR.get('WEIGHT_DECAY', {
        'BACKBONE': 0.01,
        'HEAD': 0.001
    })
    
    # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¥˜
    param_groups = []
    
    # Backbone Early Layers
    early_params = []
    for name, param in model.named_parameters():
        if param.requires_grad and any(layer in name for layer in ['conv_input', 'conv1', 'conv2']):
            early_params.append(param)
    
    if early_params:
        param_groups.append({
            'params': early_params,
            'lr': base_lr * lr_ratios['BACKBONE_EARLY'],
            'weight_decay': weight_decay_cfg['BACKBONE'],
            'name': 'backbone_early'
        })
    
    # Backbone Late Layers  
    late_params = []
    for name, param in model.named_parameters():
        if param.requires_grad and any(layer in name for layer in ['conv3', 'conv4']):
            late_params.append(param)
    
    if late_params:
        param_groups.append({
            'params': late_params,
            'lr': base_lr * lr_ratios['BACKBONE_LATE'],
            'weight_decay': weight_decay_cfg['BACKBONE'], 
            'name': 'backbone_late'
        })
    
    # Detection Head
    head_params = []
    for name, param in model.named_parameters():
        if param.requires_grad and any(head in name for head in ['dense_head', 'cls_head', 'reg_head', 'dir_head']):
            head_params.append(param)
    
    if head_params:
        param_groups.append({
            'params': head_params,
            'lr': base_lr * lr_ratios['DETECTION_HEAD'],
            'weight_decay': weight_decay_cfg['HEAD'],
            'name': 'detection_head'
        })
    
    # Other Parameters (ë‚˜ë¨¸ì§€)
    processed_params = set()
    for group in param_groups:
        for param in group['params']:
            processed_params.add(id(param))
    
    other_params = []
    for name, param in model.named_parameters():
        if param.requires_grad and id(param) not in processed_params:
            other_params.append(param)
    
    if other_params:
        param_groups.append({
            'params': other_params,
            'lr': base_lr * lr_ratios['OTHER'],
            'weight_decay': weight_decay_cfg['BACKBONE'],
            'name': 'other'
        })
    
    # Optimizer ìƒì„±
    if optim_cfg.OPTIMIZER == 'adam_onecycle':
        optimizer = torch.optim.Adam(param_groups, betas=(0.9, 0.99))
    elif optim_cfg.OPTIMIZER == 'adamw':
        optimizer = torch.optim.AdamW(param_groups)
    else:
        optimizer = torch.optim.Adam(param_groups)
    
    # ì°¨ë“± í•™ìŠµë¥  ì •ë³´ ì¶œë ¥
    print("ğŸ¯ Differential Learning Rate Applied:")
    total_params = 0
    for group in param_groups:
        param_count = sum(p.numel() for p in group['params'])
        total_params += param_count
        print(f"   - {group['name']}: LR={group['lr']:.6f}, Params={param_count:,}")
    print(f"   - Total Parameters: {total_params:,}")
    
    return optimizer