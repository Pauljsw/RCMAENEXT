# ğŸ”¥ Phase 2: Multi-scale Feature Reconstruction + Contrastive Learning

## ğŸ¯ Phase 2 ëª©í‘œ
**"CMAE-3Dì˜ í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ"**

---

## ğŸ“Š Phase 1 vs Phase 2 ì°¨ì´ì 

### **Phase 1 (í˜„ì¬ ì§„í–‰ ì¤‘):**
- âœ… Teacher-Student êµ¬ì¡° **ê¸°ë°˜ ë§ˆë ¨**
- âœ… Teacher features ì¶”ì¶œ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
- âœ… ê¸°ì¡´ R-MAE lossë§Œ ì‚¬ìš© (`TEACHER_STUDENT_WEIGHT: 0.0`)
- âœ… **ì„±ëŠ¥**: ê¸°ì¡´ R-MAEì™€ ë™ì¼ (infrastructure êµ¬ì¶•ì´ ëª©ì )

### **Phase 2 (ë‹¤ìŒ ë‹¨ê³„):**
- ğŸ”¥ Teacher featuresë¥¼ **ì‹¤ì œë¡œ í™œìš©**
- ğŸ”¥ **Multi-scale Feature Reconstruction** ì¶”ê°€
- ğŸ”¥ **Contrastive Learning** ì¶”ê°€  
- ğŸ”¥ **ì„±ëŠ¥**: +2~5 mAP í–¥ìƒ ì˜ˆìƒ

---

## ğŸ›  Phase 2ì—ì„œ êµ¬í˜„í•  ê¸°ëŠ¥ë“¤

### **1. Multi-scale Latent Feature Reconstruction (MLFR)**

**í˜„ì¬ Phase 1:**
```python
# ë‹¨ìˆœí•œ occupancy prediction
loss = BCE_loss(occupancy_pred, occupancy_target)
```

**Phase 2ì—ì„œ ì¶”ê°€:**
```python
# Multi-scale semantic feature reconstruction
teacher_features = {
    'scale_1': teacher_conv1_features,  # 16 channels
    'scale_2': teacher_conv2_features,  # 32 channels  
    'scale_3': teacher_conv3_features,  # 64 channels
    'scale_4': teacher_conv4_features   # 128 channels
}

student_features = {
    'scale_1': student_conv1_features,
    # ... ë™ì¼í•œ êµ¬ì¡°
}

# Multi-scale reconstruction loss
mlfr_loss = 0
for scale in ['scale_1', 'scale_2', 'scale_3', 'scale_4']:
    pred_features = student_decoder[scale](student_features[scale])
    target_features = teacher_features[scale].detach()
    mlfr_loss += MSE_loss(pred_features, target_features)
```

**íš¨ê³¼**: ê³ ì°¨ì› semantic ì •ë³´ í•™ìŠµìœ¼ë¡œ representation quality í–¥ìƒ

---

### **2. Voxel-level Contrastive Learning**

**Phase 2ì—ì„œ ì¶”ê°€:**
```python
def voxel_contrastive_loss(teacher_embed, student_embed, voxel_coords):
    # ê°™ì€ ê³µê°„ ìœ„ì¹˜ì˜ voxel = positive pair
    # ë‹¤ë¥¸ ê³µê°„ ìœ„ì¹˜ì˜ voxel = negative pair
    
    positive_pairs = find_same_position_voxels(voxel_coords)
    
    # InfoNCE loss
    similarity_matrix = cosine_similarity(student_embed, teacher_embed)
    contrastive_loss = info_nce_loss(similarity_matrix, positive_pairs)
    
    return contrastive_loss
```

**íš¨ê³¼**: ê³µê°„ì  ì¼ê´€ì„± í•™ìŠµìœ¼ë¡œ robust representation êµ¬ì¶•

---

### **3. Hierarchical Relational Contrastive Learning**

**Phase 2ì—ì„œ ì¶”ê°€:**
```python
# Frame-level contrastive learning (ì‹œê°„ì  ì¼ê´€ì„±)
def frame_contrastive_loss(current_features, previous_features):
    # ì¸ì ‘ í”„ë ˆì„ ê°„ semantic similarity ê°•í™”
    # False negative ë¬¸ì œ ì™„í™”
    pass

# ê±´ì„¤ì¥ë¹„ ë„ë©”ì¸ íŠ¹í™” contrastive
def domain_specific_contrastive(features, equipment_types):
    # ê°™ì€ ì¥ë¹„ íƒ€ì… = positive
    # ë‹¤ë¥¸ ì¥ë¹„ íƒ€ì… = negative  
    pass
```

---

### **4. í†µí•© Loss Function**

**Phase 1 (í˜„ì¬):**
```python
total_loss = rmae_loss  # occupancyë§Œ
```

**Phase 2:**
```python
total_loss = (
    Î»â‚ * rmae_loss +                    # ê¸°ì¡´ occupancy (1.0)
    Î»â‚‚ * mlfr_loss +                    # Multi-scale reconstruction (1.0) 
    Î»â‚ƒ * voxel_contrastive_loss +       # Voxel contrastive (0.6)
    Î»â‚„ * frame_contrastive_loss         # Frame contrastive (0.3)
)
```

**CMAE-3D ë…¼ë¬¸**: `Î»â‚ƒ = 0.6`ì´ ìµœì ê°’

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### **Phase 1 (í˜„ì¬):**
- **ëª©í‘œ**: ê¸°ì¡´ R-MAE ì„±ëŠ¥ ìœ ì§€ + infrastructure êµ¬ì¶•
- **mAP ë³€í™”**: Â±0 (ë™ì¼ ì„±ëŠ¥)

### **Phase 2:**
- **ëª©í‘œ**: CMAE-3D í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- **mAP ê°œì„ **: +2~5 mAP (CMAE-3D ë…¼ë¬¸ ê¸°ì¤€)

**ê·¼ê±°**:
- **Waymo**: CMAE-3Dê°€ ê¸°ì¡´ MAE ëŒ€ë¹„ +1.5 mAP
- **nuScenes**: CMAE-3Dê°€ ê¸°ì¡´ MAE ëŒ€ë¹„ +2.3 mAP
- **ê±´ì„¤ì¥ë¹„**: Domain-specific optimizationìœ¼ë¡œ ë” í° í–¥ìƒ ì˜ˆìƒ

---

## â° Phase 2 êµ¬í˜„ ì¼ì •

### **Step 1: Multi-scale Feature Reconstruction (1ì£¼)**
- Student decoder í™•ì¥
- Multi-scale loss êµ¬í˜„
- ì„±ëŠ¥ ê²€ì¦

### **Step 2: Voxel-level Contrastive Learning (1ì£¼)**  
- InfoNCE loss êµ¬í˜„
- Positive/negative pair ìƒì„±
- Hyperparameter tuning

### **Step 3: Frame-level + Domain-specific Contrastive (1ì£¼)**
- Memory bank êµ¬í˜„
- ê±´ì„¤ì¥ë¹„ íŠ¹í™” logic
- í†µí•© í…ŒìŠ¤íŠ¸

### **Step 4: Loss Balancing + Optimization (1ì£¼)**
- Loss weight ì¡°ì • (Î» ê°’ë“¤)
- Learning rate scheduling
- ìµœì¢… ì„±ëŠ¥ í‰ê°€

---

## ğŸ¯ Phase 1 ì™„ë£Œ í›„ ë°”ë¡œ Phase 2 ì§„í–‰

**í˜„ì¬ Pretrainingì´ ì„±ê³µì ìœ¼ë¡œ ì§„í–‰ ì¤‘**ì´ë¼ë©´:

1. âœ… **Phase 1 ì™„ë£Œ í™•ì¸**: Pretraining lossê°€ ìˆ˜ë ´í•˜ê³  ì˜¤ë¥˜ ì—†ìŒ
2. ğŸ”¥ **Phase 2 ì¦‰ì‹œ ì‹œì‘**: Multi-scale Feature Reconstruction êµ¬í˜„
3. ğŸ“Š **ì„±ëŠ¥ ë¹„êµ**: Phase 1 (ê¸°ì¡´ ì„±ëŠ¥) vs Phase 2 (í–¥ìƒëœ ì„±ëŠ¥)

**Fine-tuningì€ Phase 2 ì™„ë£Œ í›„ì— ì§„í–‰**í•´ì„œ ìµœì¢… detection ì„±ëŠ¥ì„ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤!
