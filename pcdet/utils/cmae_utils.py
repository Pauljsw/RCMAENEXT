"""
pcdet/utils/cmae_utils.py

CMAE-3D Contrastive Learning ë° Memory Management ìœ í‹¸ë¦¬í‹°

í•µì‹¬ ê¸°ëŠ¥:
1. Memory queue ê³ ê¸‰ ê´€ë¦¬
2. Positive/Negative pair ìƒì„±
3. Feature normalization ë° projection
4. Contrastive loss ê³„ì‚° í—¬í¼
5. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
6. Teacher-Student ë™ê¸°í™” ì²´í¬
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging


class MemoryQueueManager:
    """
    CMAE-3D Memory Queue ê³ ê¸‰ ê´€ë¦¬ì
    
    ë…¼ë¬¸ ê¸°ë°˜ negative samplingì„ ìœ„í•œ memory queue ê´€ë¦¬:
    - Dynamic queue size adjustment
    - Feature diversity maintenance  
    - Efficient batch update
    """
    
    def __init__(self, feature_dim: int, queue_size: int = 8192, temperature: float = 0.1):
        self.feature_dim = feature_dim
        self.queue_size = queue_size
        self.temperature = temperature
        
        # Memory queue ì´ˆê¸°í™”
        self.queue = torch.randn(feature_dim, queue_size)
        self.queue = F.normalize(self.queue, dim=0)
        self.queue_ptr = 0
        
        # Queue í’ˆì§ˆ ê´€ë¦¬
        self.diversity_threshold = 0.95  # Feature diversity ì„ê³„ê°’
        self.update_frequency = 100      # Diversity check ì£¼ê¸°
        self.update_count = 0
        
        print(f"ğŸ¯ Memory Queue Manager ì´ˆê¸°í™”")
        print(f"   - Feature dim: {feature_dim}, Queue size: {queue_size}")
        print(f"   - Temperature: {temperature}, Diversity threshold: {self.diversity_threshold}")
    
    def enqueue_batch(self, features: torch.Tensor) -> None:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ì¸ queue ì—…ë°ì´íŠ¸
        
        Args:
            features: [B, D] normalized features
        """
        batch_size = features.shape[0]
        
        # Circular queue update
        if self.queue_ptr + batch_size <= self.queue_size:
            # ì—°ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
            self.queue[:, self.queue_ptr:self.queue_ptr + batch_size] = features.T
            self.queue_ptr = (self.queue_ptr + batch_size) % self.queue_size
        else:
            # ëê¹Œì§€ ì±„ìš°ê³  ì²˜ìŒë¶€í„° ì‹œì‘
            remaining = self.queue_size - self.queue_ptr
            self.queue[:, self.queue_ptr:] = features[:remaining].T
            overflow = batch_size - remaining
            if overflow > 0:
                self.queue[:, :overflow] = features[remaining:].T
            self.queue_ptr = overflow
        
        self.update_count += 1
        
        # ì£¼ê¸°ì ìœ¼ë¡œ queue diversity ì²´í¬
        if self.update_count % self.update_frequency == 0:
            self._check_queue_diversity()
    
    def _check_queue_diversity(self) -> None:
        """
        Queueì˜ feature diversity ì²´í¬ ë° ê°œì„ 
        
        ë„ˆë¬´ ìœ ì‚¬í•œ featuresê°€ ë§ìœ¼ë©´ diversity í–¥ìƒ
        """
        # Queue ë‚´ feature ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = torch.mm(self.queue.T, self.queue)  # [Q, Q]
        
        # ëŒ€ê°ì„  ì œì™¸í•œ ìµœëŒ€ ìœ ì‚¬ë„
        mask = ~torch.eye(self.queue_size, dtype=torch.bool, device=similarities.device)
        max_similarity = similarities[mask].max().item()
        
        if max_similarity > self.diversity_threshold:
            # Diversityê°€ ë‚®ìœ¼ë©´ random noise ì¶”ê°€
            noise = torch.randn_like(self.queue) * 0.1
            self.queue = F.normalize(self.queue + noise, dim=0)
            
            print(f"   âš ï¸  Queue diversity ê°œì„ : max_sim={max_similarity:.3f} â†’ noise ì¶”ê°€")
    
    def get_negatives(self, query: torch.Tensor, exclude_indices: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Queryì— ëŒ€í•œ negative samples ë°˜í™˜
        
        Args:
            query: [B, D] query features
            exclude_indices: ì œì™¸í•  queue indices (optional)
            
        Returns:
            negatives: [B, Q, D] negative samples
        """
        # Queueì—ì„œ negative samples ì¶”ì¶œ
        negatives = self.queue.T.unsqueeze(0).expand(query.shape[0], -1, -1)  # [B, Q, D]
        
        # í•„ìš”ì‹œ íŠ¹ì • indices ì œì™¸
        if exclude_indices is not None:
            # ë³µì¡í•œ masking ë¡œì§ì€ ë‹¨ìˆœí™”
            pass
            
        return negatives
    
    def compute_contrastive_logits(self, query: torch.Tensor, positive: torch.Tensor) -> torch.Tensor:
        """
        Contrastive learning logits ê³„ì‚°
        
        Args:
            query: [B, D] query features
            positive: [B, D] positive features
            
        Returns:
            logits: [B, 1+Q] contrastive logits (positive + negatives)
        """
        batch_size = query.shape[0]
        
        # Positive similarity
        pos_sim = torch.sum(query * positive, dim=-1, keepdim=True) / self.temperature  # [B, 1]
        
        # Negative similarities
        neg_sim = torch.mm(query, self.queue) / self.temperature  # [B, Q]
        
        # Concatenate positive and negative logits
        logits = torch.cat([pos_sim, neg_sim], dim=1)  # [B, 1+Q]
        
        return logits


class ContrastivePairGenerator:
    """
    CMAE-3D Positive/Negative Pair ìƒì„±ê¸°
    
    Teacher-Student êµ¬ì¡°ì—ì„œ íš¨ê³¼ì ì¸ contrastive pair ìƒì„±:
    - Teacher features as positive targets
    - Cross-batch negative sampling
    - Hard negative mining
    """
    
    def __init__(self, hard_negative_ratio: float = 0.3):
        self.hard_negative_ratio = hard_negative_ratio
        print(f"ğŸ¯ Contrastive Pair Generator ì´ˆê¸°í™”")
        print(f"   - Hard negative ratio: {hard_negative_ratio}")
    
    def generate_teacher_student_pairs(self, 
                                     student_features: torch.Tensor,
                                     teacher_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Teacher-Student ê°„ positive pair ìƒì„±
        
        Args:
            student_features: [B, D] student features
            teacher_features: [B, D] teacher features (detached)
            
        Returns:
            queries: [B, D] student features (normalized)
            positives: [B, D] teacher features (normalized)
        """
        # Feature normalization
        queries = F.normalize(student_features, dim=-1)
        positives = F.normalize(teacher_features.detach(), dim=-1)
        
        return queries, positives
    
    def mine_hard_negatives(self, 
                          query: torch.Tensor, 
                          candidate_negatives: torch.Tensor,
                          num_hard: int) -> torch.Tensor:
        """
        Hard negative mining
        
        Queryì™€ ê°€ì¥ ìœ ì‚¬í•œ (ì–´ë ¤ìš´) negative samples ì„ íƒ
        
        Args:
            query: [B, D] query features
            candidate_negatives: [B, N, D] candidate negative features
            num_hard: ì„ íƒí•  hard negative ê°œìˆ˜
            
        Returns:
            hard_negatives: [B, num_hard, D] hard negative features
        """
        # Queryì™€ candidates ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = torch.bmm(query.unsqueeze(1), candidate_negatives.transpose(1, 2))  # [B, 1, N]
        similarities = similarities.squeeze(1)  # [B, N]
        
        # ê°€ì¥ ìœ ì‚¬í•œ (hard) negatives ì„ íƒ
        _, hard_indices = torch.topk(similarities, num_hard, dim=1)  # [B, num_hard]
        
        # Hard negatives ì¶”ì¶œ
        batch_indices = torch.arange(query.shape[0]).unsqueeze(1).expand(-1, num_hard)
        hard_negatives = candidate_negatives[batch_indices, hard_indices]  # [B, num_hard, D]
        
        return hard_negatives
    
    def create_cross_batch_negatives(self, features: torch.Tensor) -> torch.Tensor:
        """
        Cross-batch negative sampling
        
        ê°™ì€ ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ì„ negativeë¡œ ì‚¬ìš©
        
        Args:
            features: [B, D] batch features
            
        Returns:
            cross_negatives: [B, B-1, D] cross-batch negatives
        """
        batch_size = features.shape[0]
        
        # ìê¸° ìì‹  ì œì™¸í•œ ëª¨ë“  ìƒ˜í”Œì„ negativeë¡œ ì‚¬ìš©
        cross_negatives = []
        for i in range(batch_size):
            # ië²ˆì§¸ ìƒ˜í”Œ ì œì™¸
            negatives = torch.cat([features[:i], features[i+1:]], dim=0)  # [B-1, D]
            cross_negatives.append(negatives)
        
        cross_negatives = torch.stack(cross_negatives, dim=0)  # [B, B-1, D]
        
        return cross_negatives


class FeatureProjectionUtils:
    """
    CMAE-3D Feature Projection ìœ í‹¸ë¦¬í‹°
    
    Multi-scale featuresì˜ íš¨ê³¼ì ì¸ projection ë° fusion:
    - Adaptive projection based on feature scale
    - Feature fusion strategies
    - Dimensionality reduction
    """
    
    @staticmethod
    def adaptive_projection(features: Dict[str, torch.Tensor], 
                          target_dim: int = 256) -> torch.Tensor:
        """
        Multi-scale featuresì˜ adaptive projection
        
        Args:
            features: {'x_conv1': tensor, 'x_conv2': tensor, ...}
            target_dim: ëª©í‘œ projection dimension
            
        Returns:
            projected_features: [N, target_dim] projected features
        """
        projected_list = []
        
        for scale_name, feat_tensor in features.items():
            if scale_name.startswith('x_conv'):
                # Sparse tensorì˜ features ì¶”ì¶œ
                if hasattr(feat_tensor, 'features'):
                    feat = feat_tensor.features  # [N, C]
                else:
                    feat = feat_tensor
                
                # ê° scaleë³„ë¡œ adaptive pooling
                if feat.shape[1] != target_dim:
                    # Linear projectionìœ¼ë¡œ ì°¨ì› í†µì¼
                    projector = torch.nn.Linear(feat.shape[1], target_dim).to(feat.device)
                    projected = projector(feat)
                else:
                    projected = feat
                
                projected_list.append(projected)
        
        if len(projected_list) > 0:
            # Multi-scale features í‰ê·  ìœµí•©
            fused_features = torch.stack(projected_list, dim=0).mean(dim=0)  # [N, target_dim]
            return F.normalize(fused_features, dim=-1)
        else:
            return torch.empty(0, target_dim)
    
    @staticmethod
    def hierarchical_feature_fusion(features: Dict[str, torch.Tensor],
                                  fusion_strategy: str = 'weighted_avg') -> torch.Tensor:
        """
        Hierarchical multi-scale feature fusion
        
        Args:
            features: Multi-scale features dict
            fusion_strategy: 'weighted_avg', 'attention', 'concat'
            
        Returns:
            fused_features: Fused feature representation
        """
        if fusion_strategy == 'weighted_avg':
            # Scaleë³„ ê°€ì¤‘ì¹˜ (ë” ê¹Šì€ layerì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            weights = {'x_conv1': 0.1, 'x_conv2': 0.2, 'x_conv3': 0.3, 'x_conv4': 0.4}
            
            weighted_features = []
            for scale_name, feat_tensor in features.items():
                if scale_name in weights:
                    feat = feat_tensor.features if hasattr(feat_tensor, 'features') else feat_tensor
                    weighted_feat = feat * weights[scale_name]
                    weighted_features.append(weighted_feat)
            
            if weighted_features:
                return torch.cat(weighted_features, dim=-1)
            
        elif fusion_strategy == 'concat':
            # ë‹¨ìˆœ concatenation
            feat_list = []
            for scale_name, feat_tensor in features.items():
                if scale_name.startswith('x_conv'):
                    feat = feat_tensor.features if hasattr(feat_tensor, 'features') else feat_tensor
                    feat_list.append(feat)
            
            if feat_list:
                return torch.cat(feat_list, dim=-1)
        
        return torch.empty(0)


class TrainingStabilityChecker:
    """
    CMAE-3D Training Stability ì²´í¬ ìœ í‹¸ë¦¬í‹°
    
    Teacher-Student trainingì˜ ì•ˆì •ì„± ëª¨ë‹ˆí„°ë§:
    - Loss convergence tracking
    - Feature alignment monitoring  
    - EMA update quality check
    """
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.loss_history = []
        self.alignment_history = []
        
        print(f"ğŸ¯ Training Stability Checker ì´ˆê¸°í™”")
        print(f"   - Window size: {window_size}")
    
    def check_loss_stability(self, losses: Dict[str, float]) -> Dict[str, bool]:
        """
        ì†ì‹¤ í•¨ìˆ˜ ì•ˆì •ì„± ì²´í¬
        
        Args:
            losses: {'occupancy': loss, 'mlfr': loss, 'contrastive': loss}
            
        Returns:
            stability_flags: {'occupancy': stable, 'mlfr': stable, ...}
        """
        self.loss_history.append(losses)
        
        if len(self.loss_history) < self.window_size:
            return {key: True for key in losses.keys()}  # ì´ˆê¸°ì—ëŠ” ì•ˆì •ì ì´ë¼ê³  ê°€ì •
        
        # ìµœê·¼ window ë‚´ loss ë³€ë™ì„± ì²´í¬
        stability_flags = {}
        recent_losses = self.loss_history[-self.window_size:]
        
        for loss_name in losses.keys():
            loss_values = [item[loss_name] for item in recent_losses]
            
            # ë³€ë™ ê³„ìˆ˜ (CV) ê³„ì‚°
            mean_loss = np.mean(loss_values)
            std_loss = np.std(loss_values)
            cv = std_loss / (mean_loss + 1e-8)
            
            # CVê°€ 0.5 ì´í•˜ë©´ ì•ˆì •ì 
            stability_flags[loss_name] = cv < 0.5
            
            if not stability_flags[loss_name]:
                print(f"   âš ï¸  {loss_name} loss ë¶ˆì•ˆì •: CV={cv:.3f}")
        
        return stability_flags
    
    def check_teacher_student_alignment(self, 
                                      student_features: torch.Tensor,
                                      teacher_features: torch.Tensor) -> float:
        """
        Teacher-Student feature alignment ì²´í¬
        
        Args:
            student_features: [B, D] student features
            teacher_features: [B, D] teacher features
            
        Returns:
            alignment_score: 0~1 ì‚¬ì´ì˜ alignment ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        # Cosine similarity ê³„ì‚°
        student_norm = F.normalize(student_features, dim=-1)
        teacher_norm = F.normalize(teacher_features.detach(), dim=-1)
        
        cosine_sim = torch.sum(student_norm * teacher_norm, dim=-1)  # [B]
        alignment_score = cosine_sim.mean().item()
        
        self.alignment_history.append(alignment_score)
        
        # Alignment íˆìŠ¤í† ë¦¬ ê´€ë¦¬
        if len(self.alignment_history) > self.window_size:
            self.alignment_history.pop(0)
        
        # Alignment trend ì²´í¬
        if len(self.alignment_history) >= 10:
            recent_trend = np.mean(self.alignment_history[-10:])
            if recent_trend < 0.3:
                print(f"   âš ï¸  Teacher-Student alignment ë‚®ìŒ: {recent_trend:.3f}")
        
        return alignment_score
    
    def get_training_diagnostics(self) -> Dict[str, float]:
        """
        ì¢…í•©ì ì¸ training ì§„ë‹¨ ì •ë³´ ë°˜í™˜
        
        Returns:
            diagnostics: ì§„ë‹¨ ì •ë³´ dict
        """
        diagnostics = {}
        
        if len(self.loss_history) > 0:
            recent_losses = self.loss_history[-10:] if len(self.loss_history) >= 10 else self.loss_history
            
            # ê° lossì˜ í‰ê· ê³¼ ì¶”ì„¸
            for loss_name in recent_losses[0].keys():
                loss_values = [item[loss_name] for item in recent_losses]
                diagnostics[f'{loss_name}_mean'] = np.mean(loss_values)
                diagnostics[f'{loss_name}_trend'] = np.mean(np.diff(loss_values)) if len(loss_values) > 1 else 0.0
        
        if len(self.alignment_history) > 0:
            diagnostics['alignment_mean'] = np.mean(self.alignment_history[-10:])
            if len(self.alignment_history) > 1:
                diagnostics['alignment_trend'] = np.mean(np.diff(self.alignment_history[-10:]))
        
        return diagnostics


def compute_info_nce_loss(query: torch.Tensor,
                         positive: torch.Tensor, 
                         negatives: torch.Tensor,
                         temperature: float = 0.1) -> torch.Tensor:
    """
    InfoNCE loss ê³„ì‚° í—¬í¼ í•¨ìˆ˜
    
    Args:
        query: [B, D] query features
        positive: [B, D] positive features  
        negatives: [B, N, D] negative features
        temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„°
        
    Returns:
        loss: InfoNCE loss
    """
    # Positive similarity
    pos_sim = torch.sum(query * positive, dim=-1) / temperature  # [B]
    
    # Negative similarities
    neg_sim = torch.bmm(query.unsqueeze(1), negatives.transpose(1, 2)).squeeze(1) / temperature  # [B, N]
    
    # Logits: [positive, negative1, negative2, ...]
    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # [B, 1+N]
    
    # Labels: positiveëŠ” í•­ìƒ index 0
    labels = torch.zeros(query.shape[0], dtype=torch.long, device=query.device)
    
    # Cross-entropy loss
    loss = F.cross_entropy(logits, labels)
    
    return loss


def safe_normalize(tensor: torch.Tensor, dim: int = -1, eps: float = 1e-8) -> torch.Tensor:
    """
    ì•ˆì „í•œ L2 normalization (0ë²¡í„° ì²˜ë¦¬)
    
    Args:
        tensor: ì •ê·œí™”í•  tensor
        dim: ì •ê·œí™” ì°¨ì›
        eps: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ epsilon
        
    Returns:
        normalized_tensor: ì •ê·œí™”ëœ tensor
    """
    norm = torch.norm(tensor, dim=dim, keepdim=True)
    return tensor / (norm + eps)


def log_contrastive_metrics(query: torch.Tensor,
                          positive: torch.Tensor,
                          negatives: torch.Tensor) -> Dict[str, float]:
    """
    Contrastive learning ë©”íŠ¸ë¦­ ë¡œê¹…
    
    Args:
        query: [B, D] query features
        positive: [B, D] positive features
        negatives: [B, N, D] negative features
        
    Returns:
        metrics: ë¡œê¹…ìš© ë©”íŠ¸ë¦­ dict
    """
    with torch.no_grad():
        # Positive similarity
        pos_sim = torch.sum(query * positive, dim=-1).mean().item()
        
        # Negative similarity (í‰ê· )
        neg_sim = torch.bmm(query.unsqueeze(1), negatives.transpose(1, 2)).squeeze(1)  # [B, N]
        neg_sim_mean = neg_sim.mean().item()
        neg_sim_max = neg_sim.max().item()
        
        # Similarity gap (positiveì™€ negative ê°„ ì°¨ì´)
        sim_gap = pos_sim - neg_sim_mean
        
        metrics = {
            'pos_similarity': pos_sim,
            'neg_similarity_mean': neg_sim_mean,
            'neg_similarity_max': neg_sim_max,
            'similarity_gap': sim_gap,
        }
    
    return metrics