"""
pcdet/models/backbones_3d/rmae_cmae_backbone.py

R-MAE + CMAE-3D í†µí•© Backbone
- ê¸°ì¡´ ì„±ê³µí•œ R-MAE ë¡œì§ 100% ìœ ì§€ 
- CMAE-3D ë…¼ë¬¸ ë…¼ë¦¬ ì •í™•íˆ êµ¬í˜„ (Sparse CNN ì ì‘)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import spconv.pytorch as spconv
from functools import partial
import numpy as np
from .spconv_backbone_voxelnext import VoxelResBackBone8xVoxelNeXt


class RMAECMAEBackbone(VoxelResBackBone8xVoxelNeXt):
    """
    R-MAE + CMAE-3D Backbone
    
    ë…¼ë¬¸ ë…¼ë¦¬ ìœ ì§€:
    1. âœ… R-MAE radial masking (ê¸°ì¡´ ì„±ê³µ ë¡œì§)
    2. âœ… CMAE-3D Teacher-Student with proper EMA (ë…¼ë¬¸ êµ¬ì¡°)
    3. âœ… Sparse CNN ì ì‘ (Transformer ëŒ€ì‹  VoxelNeXt backbone)
    """
    
    def __init__(self, model_cfg, input_channels, grid_size, voxel_size, point_cloud_range, **kwargs):
        # âœ… ê¸°ì¡´ ì„±ê³µ ë¡œì§: ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” (ê¸°ì¡´ VoxelNeXt êµ¬ì¡° ê·¸ëŒ€ë¡œ)
        super().__init__(model_cfg, input_channels, grid_size, **kwargs)
        
        # âœ… ì‹¤ì œ ì†ì„± ì €ì¥ (Teacher êµ¬ì¶•ì— í•„ìš”)
        self.model_cfg = model_cfg
        self.input_channels = input_channels  
        self.grid_size = grid_size
        self.voxel_size = voxel_size
        self.point_cloud_range = point_cloud_range
        
        # âœ… ê¸°ì¡´ ì„±ê³µ ë¡œì§: R-MAE íŒŒë¼ë¯¸í„°
        self.masked_ratio = model_cfg.get('MASKED_RATIO', 0.8)
        self.angular_range = model_cfg.get('ANGULAR_RANGE', 1)
        
        # â• CMAE ìƒˆ íŒŒë¼ë¯¸í„°
        self.momentum = model_cfg.get('MOMENTUM', 0.999)
        self.temperature = model_cfg.get('TEMPERATURE', 0.1)
        
        # âœ… ê¸°ì¡´ ì„±ê³µ ë¡œì§: R-MAE pretrainingìš© decoder
        if hasattr(model_cfg, 'PRETRAINING') and model_cfg.PRETRAINING:
            norm_fn = partial(nn.BatchNorm1d, eps=1e-3, momentum=0.01)
            self.occupancy_decoder = spconv.SparseSequential(
                spconv.SubMConv3d(128, 64, 3, padding=1, bias=False, indice_key='dec1'),
                norm_fn(64), nn.ReLU(),
                spconv.SubMConv3d(64, 32, 3, padding=1, bias=False, indice_key='dec2'),
                norm_fn(32), nn.ReLU(),
                spconv.SubMConv3d(32, 1, 1, bias=True, indice_key='dec_out')
            )
            
            # â• CMAE ìƒˆ ìš”ì†Œ: Teacher network ì¶”ê°€
            self._build_teacher_network()
            
            # â• CMAE ìƒˆ ìš”ì†Œ: Feature projectors ì¶”ê°€
            self._build_feature_projectors()
        
        print(f"ğŸ¯ R-MAE + CMAE Backbone ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - R-MAE mask ratio: {self.masked_ratio}")
        print(f"   - CMAE momentum: {self.momentum}")
    
    def _build_teacher_network(self):
        """
        â• CMAE Teacher network êµ¬ì¶• (ë…¼ë¬¸ ë…¼ë¦¬ ì¤€ìˆ˜)
        
        CMAE-3D ë…¼ë¬¸:
        - Teacher: full point cloud (unmasked)
        - Student: masked point cloud  
        - TeacherëŠ” Studentì˜ EMA copy
        """
        try:
            print("ğŸ”§ CMAE-3D Teacher network êµ¬ì¶• (ë…¼ë¬¸ ë…¼ë¦¬ ì¤€ìˆ˜)")
            print(f"ğŸ” Teacher íŒŒë¼ë¯¸í„°: input_channels={self.input_channels}, grid_size={self.grid_size}")
            
            # âœ… CMAE-3D ë…¼ë¬¸: TeacherëŠ” Studentì™€ ë™ì¼í•œ êµ¬ì¡°
            # Sparse CNN ì ì‘: VoxelNeXt backbone ì‚¬ìš© (Transformer ëŒ€ì‹ )
            from .spconv_backbone_voxelnext import VoxelResBackBone8xVoxelNeXt
            
            # Teacherìš© ë³„ë„ model_cfg (indice_key ì¶©ëŒ ë°©ì§€)
            teacher_cfg = type('TeacherConfig', (), {})()
            for attr in dir(self.model_cfg):
                if not attr.startswith('_'):
                    setattr(teacher_cfg, attr, getattr(self.model_cfg, attr))
            
            self.teacher_backbone = VoxelResBackBone8xVoxelNeXt(
                model_cfg=teacher_cfg,
                input_channels=self.input_channels, 
                grid_size=self.grid_size
            )
            
            # âœ… CMAE-3D ë…¼ë¬¸: Teacher íŒŒë¼ë¯¸í„°ë¥¼ Studentë¡œ ì´ˆê¸°í™”
            print("ğŸ”„ Teacher íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” (CMAE-3D EMA ë°©ì‹)")
            self._initialize_teacher_from_student()
            
            # TeacherëŠ” gradient ì—…ë°ì´íŠ¸ ì—†ìŒ (EMAë§Œ)
            for param in self.teacher_backbone.parameters():
                param.requires_grad = False
            
            self.has_teacher = True
            print(f"âœ… CMAE-3D Teacher network êµ¬ì¶• ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Teacher network êµ¬ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            self.has_teacher = False
    
    def _initialize_teacher_from_student(self):
        """
        CMAE-3D ë…¼ë¬¸: Teacherë¥¼ Student íŒŒë¼ë¯¸í„°ë¡œ ì •í™•íˆ ì´ˆê¸°í™”
        """
        with torch.no_grad():
            # ëª¨ë“  conv ë ˆì´ì–´ ì´ˆê¸°í™”
            teacher_modules = list(self.teacher_backbone.named_modules())
            student_modules = list(self.named_modules())
            
            teacher_dict = {name: module for name, module in teacher_modules}
            student_dict = {name: module for name, module in student_modules}
            
            # conv_input, conv1, conv2, conv3, conv4 ì´ˆê¸°í™”
            layer_names = ['conv_input', 'conv1', 'conv2', 'conv3', 'conv4']
            
            for layer_name in layer_names:
                if layer_name in teacher_dict and layer_name in student_dict:
                    self._copy_layer_parameters(
                        student_dict[layer_name], 
                        teacher_dict[layer_name]
                    )
                    print(f"âœ… Teacher {layer_name} ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _copy_layer_parameters(self, student_layer, teacher_layer):
        """ê°œë³„ ë ˆì´ì–´ íŒŒë¼ë¯¸í„° ë³µì‚¬"""
        student_params = list(student_layer.parameters())
        teacher_params = list(teacher_layer.parameters())
        
        for s_param, t_param in zip(student_params, teacher_params):
            if s_param.shape == t_param.shape:
                t_param.data.copy_(s_param.data)
    
    def _build_feature_projectors(self):
        """
        â• CMAE Feature projectors êµ¬ì¶• (ìƒˆë¡œ ì¶”ê°€)
        Multi-scale featuresë¥¼ ê³µí†µ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜
        """
        try:
            # ê°„ë‹¨í•œ projection heads
            self.student_projector = nn.Sequential(
                nn.Linear(128, 128),  # VoxelNeXt ë§ˆì§€ë§‰ ì±„ë„
                nn.ReLU(),
                nn.Linear(128, 256)   # ê³µí†µ projection ì°¨ì›
            )
            
            self.teacher_projector = nn.Sequential(
                nn.Linear(128, 128),
                nn.ReLU(), 
                nn.Linear(128, 256)
            )
            
            # Teacher projectorë¥¼ Studentë¡œ ì´ˆê¸°í™”
            with torch.no_grad():
                for teacher_param, student_param in zip(
                    self.teacher_projector.parameters(),
                    self.student_projector.parameters()
                ):
                    teacher_param.copy_(student_param)
                    teacher_param.requires_grad = False
            
            print("âœ… Feature projectors êµ¬ì¶• ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ Feature projectors êµ¬ì¶• ì‹¤íŒ¨: {e}")
    
    def forward(self, batch_dict):
        """
        âœ… CMAE-3D + R-MAE í†µí•© forward (ë…¼ë¬¸ ë…¼ë¦¬ ì¤€ìˆ˜)
        
        CMAE-3D ë…¼ë¬¸:
        - Student: masked input â†’ masked features
        - Teacher: full input â†’ full features  
        - Contrastive learning between student & teacher
        """
        voxel_features = batch_dict['voxel_features']
        voxel_coords = batch_dict['voxel_coords']
        batch_size = batch_dict['batch_size']
        
        # âœ… R-MAE masking ì ìš© (training ì‹œì—ë§Œ)
        if self.training and hasattr(self.model_cfg, 'PRETRAINING') and self.model_cfg.PRETRAINING:
            batch_dict['original_voxel_coords'] = voxel_coords.clone()
            batch_dict['original_voxel_features'] = voxel_features.clone()
            
            # R-MAE radial masking
            voxel_coords, voxel_features = self.radial_masking(voxel_coords, voxel_features)
            batch_dict['voxel_coords'] = voxel_coords
            batch_dict['voxel_features'] = voxel_features
            
            # âœ… CMAE-3D: Teacher forward (full/unmasked input)
            if hasattr(self, 'has_teacher') and self.has_teacher:
                teacher_features = self._forward_teacher_correct(
                    batch_dict['original_voxel_coords'], 
                    batch_dict['original_voxel_features'], 
                    batch_size
                )
                batch_dict['teacher_features'] = teacher_features
        
        # âœ… Student network forward (masked input)
        input_sp_tensor = spconv.SparseConvTensor(
            features=voxel_features,
            indices=voxel_coords.int(),
            spatial_shape=self.sparse_shape,
            batch_size=batch_size
        )
        
        # VoxelNeXt conv layers
        x = self.conv_input(input_sp_tensor)
        x_conv1 = self.conv1(x)
        x_conv2 = self.conv2(x_conv1)  
        x_conv3 = self.conv3(x_conv2)
        x_conv4 = self.conv4(x_conv3)
        
        # âœ… Pretraining outputs
        if self.training and hasattr(self.model_cfg, 'PRETRAINING') and self.model_cfg.PRETRAINING:
            # R-MAE occupancy prediction
            occupancy_pred = self.occupancy_decoder(x_conv4)
            batch_dict['occupancy_pred'] = occupancy_pred.features
            batch_dict['occupancy_coords'] = occupancy_pred.indices
            
            # âœ… CMAE-3D: Student features
            student_features = self._extract_global_features_correct(x_conv4)
            batch_dict['student_features'] = student_features
                
            # âœ… CMAE-3D: Teacher EMA ì—…ë°ì´íŠ¸
            if hasattr(self, 'has_teacher') and self.has_teacher:
                self._update_teacher_ema_correct()
            
            # Multi-scale features
            batch_dict['multi_scale_features'] = {
                'x_conv1': x_conv1, 'x_conv2': x_conv2,
                'x_conv3': x_conv3, 'x_conv4': x_conv4,
            }
        
        # âœ… ê¸°ì¡´ VoxelNeXt ì¶œë ¥ í˜•ì‹ ìœ ì§€
        batch_dict.update({
            'encoded_spconv_tensor': x_conv4,
            'encoded_spconv_tensor_stride': 8,
            'multi_scale_3d_features': {
                'x_conv1': x_conv1, 'x_conv2': x_conv2,
                'x_conv3': x_conv3, 'x_conv4': x_conv4,
            }
        })
        
        return batch_dict
    
    def radial_masking(self, voxel_coords, voxel_features):
        """
        âœ… ê¸°ì¡´ ì„±ê³µí•œ R-MAE radial masking ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        """
        if not self.training:
            return voxel_coords, voxel_features
            
        batch_size = int(voxel_coords[:, 0].max()) + 1
        masked_coords, masked_features = [], []
        
        for batch_idx in range(batch_size):
            mask = voxel_coords[:, 0] == batch_idx
            if not mask.any():
                continue
                
            coords_b = voxel_coords[mask]
            features_b = voxel_features[mask]
            
            # R-MAE ê°ë„ ê¸°ë°˜ ë§ˆìŠ¤í‚¹
            xyz = coords_b[:, 1:4].float()
            
            # Cylindrical coordinates
            x, y = xyz[:, 0], xyz[:, 1]
            theta = torch.atan2(y, x)
            
            # Angular groups
            num_groups = int(360 / self.angular_range)
            group_size = 2 * np.pi / num_groups
            theta_norm = (theta + np.pi) % (2 * np.pi)
            groups = (theta_norm / group_size).long()
            
            # Random group selection for masking
            groups_to_mask = torch.randperm(num_groups)[:int(num_groups * self.masked_ratio)]
            
            # Keep voxels NOT in masked groups
            keep_mask = torch.ones(len(coords_b), dtype=torch.bool, device=coords_b.device)
            for group_idx in groups_to_mask:
                group_mask = groups == group_idx
                keep_mask = keep_mask & (~group_mask)
            
            masked_coords.append(coords_b[keep_mask])
            masked_features.append(features_b[keep_mask])
        
        if masked_coords:
            return torch.cat(masked_coords, dim=0), torch.cat(masked_features, dim=0)
        else:
            return voxel_coords, voxel_features
    
    def _forward_teacher_correct(self, original_coords, original_features, batch_size):
        """
        â• CMAE Teacher network forward (ë…¼ë¬¸ ë…¼ë¦¬ ì¤€ìˆ˜)
        
        CMAE-3D ë…¼ë¬¸:
        - TeacherëŠ” full/unmasked input ì²˜ë¦¬
        - Studentì™€ ë™ì¼í•œ forward pass
        """
        if not (hasattr(self, 'has_teacher') and self.has_teacher):
            # Teacher ì—†ìœ¼ë©´ Student features ì‚¬ìš© (ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ° ê²½ìš° ì—†ìŒ)
            return torch.randn(1, 256, device=original_features.device)
        
        try:
            with torch.no_grad():  # TeacherëŠ” gradient ê³„ì‚° ì—†ìŒ
                # âœ… CMAE-3D: Teacher forward (unmasked input)
                input_sp_tensor = spconv.SparseConvTensor(
                    features=original_features,
                    indices=original_coords.int(),
                    spatial_shape=self.sparse_shape,
                    batch_size=batch_size
                )
                
                # Teacher VoxelNeXt forward
                t_x = self.teacher_backbone.conv_input(input_sp_tensor)
                t_x_conv1 = self.teacher_backbone.conv1(t_x)
                t_x_conv2 = self.teacher_backbone.conv2(t_x_conv1)
                t_x_conv3 = self.teacher_backbone.conv3(t_x_conv2)
                t_x_conv4 = self.teacher_backbone.conv4(t_x_conv3)
                
                # Global features ì¶”ì¶œ
                teacher_features = self._extract_global_features_correct(t_x_conv4)
                
                return teacher_features
                
        except Exception as e:
            print(f"âš ï¸ Teacher forward ì‹¤íŒ¨: {e}")
            return torch.randn(1, 256, device=original_features.device)
    
    def _extract_global_features_correct(self, sparse_tensor):
        """
        â• CMAE Global feature extraction (ë…¼ë¬¸ ì¤€ìˆ˜)
        """
        try:
            if hasattr(sparse_tensor, 'features'):
                features = sparse_tensor.features
            else:
                features = sparse_tensor
            
            if features.size(0) == 0:
                return torch.randn(1, 256, device=features.device)
            
            # Global average pooling
            global_feat = torch.mean(features, dim=0, keepdim=True)
            
            # Project to target dimension (ë…¼ë¬¸ì—ì„œëŠ” íŠ¹ì • ì°¨ì›ìœ¼ë¡œ projection)
            if hasattr(self, 'student_projector'):
                projected = self.student_projector(global_feat)
            else:
                # Fallback: ì°¨ì› ë§ì¶”ê¸°
                if global_feat.size(-1) != 256:
                    if global_feat.size(-1) >= 256:
                        projected = global_feat[:, :256]
                    else:
                        padding = 256 - global_feat.size(-1)
                        projected = F.pad(global_feat, (0, padding))
                else:
                    projected = global_feat
            
            return projected
            
        except Exception as e:
            print(f"âš ï¸ Global feature extraction ì‹¤íŒ¨: {e}")
            device = sparse_tensor.features.device if hasattr(sparse_tensor, 'features') else sparse_tensor.device
            return torch.randn(1, 256, device=device)
    
    def _update_teacher_ema_correct(self):
        """
        â• CMAE Teacher EMA update (ë…¼ë¬¸ ë…¼ë¦¬ ì¤€ìˆ˜)
        
        CMAE-3D ë…¼ë¬¸: Î¸_teacher = momentum * Î¸_teacher + (1-momentum) * Î¸_student
        """
        if not (hasattr(self, 'has_teacher') and self.has_teacher):
            return
        
        try:
            with torch.no_grad():
                # âœ… CMAE-3D EMA ì—…ë°ì´íŠ¸ (momentum=0.999)
                teacher_modules = dict(self.teacher_backbone.named_modules())
                student_modules = dict(self.named_modules())
                
                # conv layers EMA ì—…ë°ì´íŠ¸
                layer_names = ['conv_input', 'conv1', 'conv2', 'conv3', 'conv4']
                
                for layer_name in layer_names:
                    if layer_name in teacher_modules and layer_name in student_modules:
                        self._ema_update_layer_correct(
                            student_modules[layer_name],
                            teacher_modules[layer_name]
                        )
                
                # Feature projectors EMA ì—…ë°ì´íŠ¸
                if hasattr(self, 'teacher_projector') and hasattr(self, 'student_projector'):
                    for t_param, s_param in zip(
                        self.teacher_projector.parameters(),
                        self.student_projector.parameters()
                    ):
                        t_param.data.mul_(self.momentum).add_(
                            s_param.data, alpha=1.0 - self.momentum
                        )
                        
        except Exception as e:
            print(f"âš ï¸ Teacher EMA update ì‹¤íŒ¨: {e}")
    
    def _ema_update_layer_correct(self, student_layer, teacher_layer):
        """CMAE-3D ë…¼ë¬¸ ê¸°ì¤€ EMA ì—…ë°ì´íŠ¸"""
        try:
            student_params = list(student_layer.parameters())
            teacher_params = list(teacher_layer.parameters())
            
            for s_param, t_param in zip(student_params, teacher_params):
                if s_param.shape == t_param.shape:
                    # CMAE-3D EMA: Î¸_t = m*Î¸_t + (1-m)*Î¸_s
                    t_param.data.mul_(self.momentum).add_(
                        s_param.data, alpha=1.0 - self.momentum
                    )
                        
        except Exception as e:
            print(f"âš ï¸ Layer EMA ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


# âœ… ëª¨ë¸ ë“±ë¡ì„ ìœ„í•œ alias
RMAECMAEBackbone = RMAECMAEBackbone