# pcdet/models/backbones_3d/rmae_cmae_backbone_phase2.py
"""
CMAE-3D Phase 2: Multi-scale Latent Feature Reconstruction Backbone

Phase 1 ê¸°ëŠ¥ + Phase 2 MLFR ì¶”ê°€:
- Teacher-Student Architecture (Phase 1ì—ì„œ êµ¬ì¶•ë¨)
- Multi-scale Latent Feature Reconstruction (ìƒˆë¡œ ì¶”ê°€)
- ê¸°ì¡´ R-MAE ì™„ì „ í˜¸í™˜ ìœ ì§€
"""

import torch
import torch.nn as nn
import spconv.pytorch as spconv
from functools import partial
import numpy as np

# Phase 1 backbone ìƒì†
from .rmae_cmae_backbone_phase1 import RMAECMAEBackbonePhase1
from .components.multiscale_feature_decoder import MultiScaleFeatureDecoder
from .components.voxel_contrastive_loss import VoxelContrastiveLoss


class RMAECMAEBackbonePhase2(RMAECMAEBackbonePhase1):
    """
    ğŸ”¥ Phase 2: Multi-scale Latent Feature Reconstruction Backbone
    
    Phase 1 ê¸°ëŠ¥ ì™„ì „ ìœ ì§€ + CMAE-3D MLFR ì¶”ê°€:
    
    Phase 1 (ê¸°ì¡´):
    - Teacher-Student Architecture âœ…
    - R-MAE occupancy prediction âœ…
    - Basic feature extraction âœ…
    
    Phase 2 (ìƒˆë¡œ ì¶”ê°€):
    - Multi-scale Latent Feature Reconstruction ğŸ”¥
    - Semantic feature reconstruction ğŸ”¥  
    - Enhanced loss function ğŸ”¥
    """
    
    def __init__(self, model_cfg, input_channels, grid_size, voxel_size, point_cloud_range, **kwargs):
        # Phase 1 ì´ˆê¸°í™”
        super().__init__(model_cfg, input_channels, grid_size, voxel_size, point_cloud_range, **kwargs)
        
        # ğŸ“ Phase 2 MLFR ì„¤ì •
        self.enable_mlfr = model_cfg.get('ENABLE_MLFR', True)
        self.mlfr_weight = model_cfg.get('MLFR_WEIGHT', 1.0)
        
        # ğŸ“ Phase 2 Voxel Contrastive Learning ì„¤ì •
        self.enable_voxel_contrastive = model_cfg.get('ENABLE_VOXEL_CONTRASTIVE', True)
        self.voxel_contrastive_weight = model_cfg.get('VOXEL_CONTRASTIVE_WEIGHT', 0.6)  # CMAE-3D ë…¼ë¬¸ ê¸°ë³¸ê°’
        
        # ğŸ“ Phase 2 Step 3: Frame Contrastive Learning ì„¤ì •
        self.enable_frame_contrastive = model_cfg.get('ENABLE_FRAME_CONTRASTIVE', True)
        self.frame_contrastive_weight = model_cfg.get('FRAME_CONTRASTIVE_WEIGHT', 0.3)  # CMAE-3D ë…¼ë¬¸ ê¸°ë³¸ê°’
        
        # MLFR í™œì„±í™” ì‹œì—ë§Œ decoder ìƒì„±
        if self.enable_mlfr and self.enable_teacher_student:
            self.mlfr_decoder = MultiScaleFeatureDecoder(model_cfg.get('MLFR_CONFIG', {}))
            print(f"ğŸ”¥ Phase 2 MLFR enabled with weight: {self.mlfr_weight}")
        else:
            self.mlfr_decoder = None
            print(f"ğŸ¯ Phase 2 MLFR disabled (enable_mlfr: {self.enable_mlfr}, teacher_student: {self.enable_teacher_student})")
        
        # Voxel Contrastive Learning í™œì„±í™” ì‹œì—ë§Œ loss module ìƒì„±
        if self.enable_voxel_contrastive and self.enable_teacher_student:
            voxel_contrastive_cfg = model_cfg.get('VOXEL_CONTRASTIVE_CONFIG', {})
            voxel_contrastive_cfg.update({
                'FEATURE_DIM': 128,  # Final feature dimension
                'PROJECTION_DIM': 128,  # Contrastive projection dimension
                'CONTRASTIVE_TEMPERATURE': 0.07  # CMAE-3D paper default
            })
            self.voxel_contrastive_loss = VoxelContrastiveLoss(voxel_contrastive_cfg)
            print(f"ğŸ”¥ Phase 2 Voxel Contrastive enabled with weight: {self.voxel_contrastive_weight}")
        else:
            self.voxel_contrastive_loss = None
            print(f"ğŸ¯ Phase 2 Voxel Contrastive disabled (enable: {self.enable_voxel_contrastive}, teacher_student: {self.enable_teacher_student})")
        
        # Frame Contrastive Learning í™œì„±í™” ì‹œì—ë§Œ loss module ìƒì„±
        if self.enable_frame_contrastive and self.enable_teacher_student:
            frame_contrastive_cfg = model_cfg.get('FRAME_CONTRASTIVE_CONFIG', {})
            frame_contrastive_cfg.update({
                'FEATURE_DIM': 128,  # Final feature dimension
                'PROJECTION_DIM': 128,  # Frame contrastive projection dimension
                'FRAME_TEMPERATURE': 0.1,  # Frame-level temperature (ë‚®ê²Œ ì„¤ì •)
                'MEMORY_BANK_SIZE': 16,  # ìµœê·¼ 16 í”„ë ˆì„ ì €ì¥
                'MOMENTUM_UPDATE': 0.99,  # Momentum update
                'ENABLE_DOMAIN_SPECIFIC': True,  # ê±´ì„¤ì¥ë¹„ ë„ë©”ì¸ íŠ¹í™”
                'EQUIPMENT_TYPES': ['dumptruck', 'excavator', 'grader', 'roller']
            })
            
            from .components.frame_contrastive_loss import FrameContrastiveLoss
            self.frame_contrastive_loss = FrameContrastiveLoss(frame_contrastive_cfg)
            print(f"ğŸ”¥ Phase 2 Step 3 Frame Contrastive enabled with weight: {self.frame_contrastive_weight}")
        else:
            self.frame_contrastive_loss = None
            print(f"ğŸ¯ Phase 2 Step 3 Frame Contrastive disabled (enable: {self.enable_frame_contrastive}, teacher_student: {self.enable_teacher_student})")
        
        # Phase 2 forward return dict
        self.forward_ret_dict = {}
        
        print(f"ğŸš€ Phase 2 Step 3 Backbone initialized:")
        print(f"   - Phase 1 features: âœ… Teacher-Student, R-MAE")
        print(f"   - Phase 2 features: {'âœ…' if self.enable_mlfr else 'âŒ'} MLFR, {'âœ…' if self.enable_voxel_contrastive else 'âŒ'} Voxel Contrastive, {'âœ…' if self.enable_frame_contrastive else 'âŒ'} Frame Contrastive")
    
    def teacher_forward(self, batch_dict):
        """
        ğŸ”¥ Phase 2 Enhanced Teacher Forward
        
        Phase 1 teacher_forward í™•ì¥:
        - ê¸°ì¡´: Multi-scale feature ì¶”ì¶œ
        - ì¶”ê°€: MLFRì„ ìœ„í•œ ë” ì •í™•í•œ feature alignment
        """
        voxel_features = batch_dict['voxel_features']
        voxel_coords = batch_dict['voxel_coords']
        batch_size = batch_dict['batch_size']
        
        # VoxelNeXt backbone forward (ì™„ì „í•œ view)
        input_sp_tensor = spconv.SparseConvTensor(
            features=voxel_features,
            indices=voxel_coords.int(),
            spatial_shape=self.sparse_shape,
            batch_size=batch_size
        )
        
        # Multi-scale feature extraction
        x = self.conv_input(input_sp_tensor)
        x_conv1 = self.conv1(x)      # Scale 1: 16 channels
        x_conv2 = self.conv2(x_conv1) # Scale 2: 32 channels  
        x_conv3 = self.conv3(x_conv2) # Scale 3: 64 channels
        x_conv4 = self.conv4(x_conv3) # Scale 4: 128 channels
        
        # ğŸ“ Phase 2: MLFRì„ ìœ„í•œ ì •í™•í•œ multi-scale features
        teacher_features = {
            'scale_1': x_conv1,  # SparseConvTensor
            'scale_2': x_conv2,  # SparseConvTensor
            'scale_3': x_conv3,  # SparseConvTensor
            'scale_4': x_conv4,  # SparseConvTensor
            'final_tensor': x_conv4,  # Backward compatibility
            
            # ì¶”ê°€: MLFR debuggingì„ ìœ„í•œ ì •ë³´
            'scale_info': {
                'scale_1': {'channels': 16, 'num_voxels': x_conv1.features.size(0)},
                'scale_2': {'channels': 32, 'num_voxels': x_conv2.features.size(0)},
                'scale_3': {'channels': 64, 'num_voxels': x_conv3.features.size(0)},
                'scale_4': {'channels': 128, 'num_voxels': x_conv4.features.size(0)}
            }
        }
        
        return teacher_features
    
    def student_forward(self, batch_dict):
        """
        ğŸ”¥ Phase 2 Student Forward (ìˆ˜ì •ë¨ - super() í˜¸ì¶œ ì œê±°)
        
        ì§ì ‘ R-MAE masked forward ìˆ˜í–‰:
        - R-MAE masked forward ì²˜ë¦¬
        - MLFRì„ ìœ„í•œ multi-scale feature ì¶”ì¶œ
        """
        
        # ğŸ“ 1. R-MAE ê¸°ë³¸ forward ìˆ˜í–‰ (Phase 1 backbone ë¡œì§ í™œìš©)
        voxel_features = batch_dict['voxel_features']
        voxel_coords = batch_dict['voxel_coords'] 
        batch_size = batch_dict['batch_size']
        
        # ğŸ“ 2. Sparse Conv forward (VoxelNeXt ë¡œì§)
        input_sp_tensor = spconv.SparseConvTensor(
            features=voxel_features,
            indices=voxel_coords.int(),
            spatial_shape=self.sparse_shape,
            batch_size=batch_size
        )
        
        # Multi-scale feature extraction
        x = self.conv_input(input_sp_tensor)
        x_conv1 = self.conv1(x)      # Scale 1: 16 channels
        x_conv2 = self.conv2(x_conv1) # Scale 2: 32 channels  
        x_conv3 = self.conv3(x_conv2) # Scale 3: 64 channels
        x_conv4 = self.conv4(x_conv3) # Scale 4: 128 channels
        
        # ğŸ“ 3. Occupancy prediction (R-MAE pretraining)
        if self.training and hasattr(self.model_cfg, 'PRETRAINING') and self.model_cfg.PRETRAINING:
            if hasattr(self, 'occupancy_decoder'):
                occupancy_pred = self.occupancy_decoder(x_conv4)
                batch_dict['occupancy_pred'] = occupancy_pred.features
                batch_dict['occupancy_coords'] = occupancy_pred.indices
        
        # ğŸ“ 4. Standard VoxelNeXt output format
        result = {
            'encoded_spconv_tensor': x_conv4,
            'encoded_spconv_tensor_stride': 8,
            'multi_scale_3d_features': {
                'x_conv1': x_conv1, 'x_conv2': x_conv2,
                'x_conv3': x_conv3, 'x_conv4': x_conv4,
            }
        }
        
        # ğŸ“ 5. Phase 2: MLFRì„ ìœ„í•œ student multi-scale features ì¶”ê°€
        if self.enable_mlfr:
            result['student_multiscale_features'] = {
                'scale_1': x_conv1,
                'scale_2': x_conv2,
                'scale_3': x_conv3,
                'scale_4': x_conv4
            }
            
            # Scale ì •ë³´ ì €ì¥ (debugging)
            result['student_scale_info'] = {
                'scale_1': {'channels': 16, 'num_voxels': x_conv1.features.size(0)},
                'scale_2': {'channels': 32, 'num_voxels': x_conv2.features.size(0)},
                'scale_3': {'channels': 64, 'num_voxels': x_conv3.features.size(0)},
                'scale_4': {'channels': 128, 'num_voxels': x_conv4.features.size(0)}
            }
        
        return result
    
    def forward(self, batch_dict):
        """
        ğŸš€ Phase 2 Step 3 Forward: Teacher-Student + MLFR + Voxel + Frame Contrastive í†µí•© (ìˆ˜ì •ë¨)
        """
        
        if not self.enable_teacher_student or not self.training:
            # Teacher-Student ë¹„í™œì„±í™” ì‹œ: ê¸°ì¡´ Phase 1 ë™ì‘
            # RadialMAEVoxelNeXtì˜ forwardë¥¼ ì§ì ‘ í˜¸ì¶œ
            return super(RMAECMAEBackbonePhase1, self).forward(batch_dict)
        
        # ğŸ“ Phase 2: Enhanced Teacher-Student + MLFR Training Mode
        
        # 1. Teacher Branch: Complete view ì²˜ë¦¬
        teacher_batch = {
            'voxel_features': batch_dict['voxel_features'].clone(),
            'voxel_coords': batch_dict['voxel_coords'].clone(),
            'batch_size': batch_dict['batch_size']
        }
        teacher_features = self.teacher_forward(teacher_batch)
        
        # 2. Student Branch: Masked view ì²˜ë¦¬ + multi-scale features
        student_result = self.student_forward(batch_dict)
        
        # 3. ğŸ“ Phase 2: Multi-scale Latent Feature Reconstruction
        mlfr_results = {}
        if self.enable_mlfr and self.mlfr_decoder is not None:
            if 'student_multiscale_features' in student_result:
                
                student_multiscale = student_result['student_multiscale_features']
                mlfr_results = self.mlfr_decoder(student_multiscale, teacher_features)
                
                print(f"ğŸ”¥ MLFR executed: {len(mlfr_results['mlfr_losses'])} scales, " +
                      f"total_loss={mlfr_results['total_mlfr_loss']:.6f}")
            else:
                print("âš ï¸ MLFR skipped: student_multiscale_features not found")
                mlfr_results = {
                    'reconstructed_features': {},
                    'mlfr_losses': {},
                    'total_mlfr_loss': torch.tensor(0.0, device='cuda', requires_grad=True)
                }
        
        # 4. ğŸ“ Phase 2: Voxel Contrastive Learning
        voxel_contrastive_results = {}
        if self.enable_voxel_contrastive and self.voxel_contrastive_loss is not None:
            
            # Teacherì™€ Studentì˜ final features ì¶”ì¶œ
            teacher_final = teacher_features.get('final_tensor')
            student_final = student_result.get('encoded_spconv_tensor')
            
            if teacher_final is not None and student_final is not None:
                # Featuresì™€ coordinates ì¶”ì¶œ
                teacher_feat = teacher_final.features  # [N_t, 128]
                teacher_coords = teacher_final.indices  # [N_t, 4]
                student_feat = student_final.features   # [N_s, 128] 
                student_coords = student_final.indices  # [N_s, 4]
                
                # Voxel contrastive learning ì‹¤í–‰
                voxel_contrastive_results = self.voxel_contrastive_loss(
                    teacher_feat, student_feat, teacher_coords, student_coords
                )
                
                print(f"ğŸ”¥ Voxel Contrastive executed: {voxel_contrastive_results['num_positive_pairs']} positives, " +
                      f"acc={voxel_contrastive_results['contrastive_acc']:.3f}")
            else:
                print("âš ï¸ Voxel Contrastive skipped: missing teacher or student features")
                voxel_contrastive_results = {
                    'voxel_contrastive_loss': torch.tensor(0.0, device='cuda', requires_grad=True),
                    'num_positive_pairs': 0,
                    'num_negative_pairs': 0,
                    'contrastive_acc': 0.0
                }
        
        # 5. ğŸ“ Phase 2: Frame Contrastive Learning (ìˆ˜ì •ë¨)
        frame_contrastive_results = {}
        if self.enable_frame_contrastive and self.frame_contrastive_loss is not None:
            
            # Frame contrastiveë¥¼ ìœ„í•œ features ì¶”ì¶œ
            if 'encoded_spconv_tensor' in student_result:
                final_tensor = student_result['encoded_spconv_tensor']
                frame_features = final_tensor.features  # [N, 128]
                frame_coords = final_tensor.indices  # [N, 4] (batch, z, y, x)
                
                # Batchë³„ë¡œ features pooling (frame-level representation)
                batch_size = batch_dict['batch_size']
                pooled_features = []
                timestamps = []
                equipment_types = []
                
                for b in range(batch_size):
                    batch_mask = frame_coords[:, 0] == b
                    if batch_mask.sum() > 0:
                        # Global average pooling for frame-level representation
                        batch_features = frame_features[batch_mask]  # [N_b, 128]
                        pooled_feature = batch_features.mean(dim=0, keepdim=True)  # [1, 128]
                        pooled_features.append(pooled_feature)
                        
                        # ğŸ“ ìˆ˜ì •: Frame timestamp ì²˜ë¦¬ (ë¬¸ìì—´ ì²˜ë¦¬)
                        try:
                            if 'frame_id' in batch_dict:
                                frame_id = batch_dict['frame_id']
                                if isinstance(frame_id, (list, tuple)):
                                    # List/tupleì¸ ê²½ìš° batch indexì— í•´ë‹¹í•˜ëŠ” ê°’ ì¶”ì¶œ
                                    if b < len(frame_id):
                                        timestamp_raw = frame_id[b]
                                    else:
                                        timestamp_raw = b  # Fallback
                                else:
                                    # Single valueì¸ ê²½ìš°
                                    timestamp_raw = frame_id
                                
                                # ë¬¸ìì—´ì¸ ê²½ìš° ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
                                if isinstance(timestamp_raw, str):
                                    # íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: "000001.bin" -> 1)
                                    import re
                                    numbers = re.findall(r'\d+', timestamp_raw)
                                    if numbers:
                                        frame_timestamp = float(numbers[-1])  # ë§ˆì§€ë§‰ ìˆ«ì ì‚¬ìš©
                                    else:
                                        frame_timestamp = float(b)  # Fallback to batch index
                                else:
                                    frame_timestamp = float(timestamp_raw)
                            else:
                                # frame_idê°€ ì—†ìœ¼ë©´ batch index ì‚¬ìš©
                                frame_timestamp = float(b)
                        except Exception as e:
                            print(f"Warning: Frame timestamp extraction failed: {e}, using batch index")
                            frame_timestamp = float(b)
                        
                        timestamps.append(frame_timestamp)
                        
                        # Equipment type (gt_boxesì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
                        equipment_type = self._get_dominant_equipment_type(batch_dict, b)
                        equipment_types.append(equipment_type)
                
                if len(pooled_features) > 0:
                    pooled_features = torch.cat(pooled_features, dim=0)  # [B, 128]
                    timestamps = torch.tensor(timestamps, device=pooled_features.device, dtype=torch.float32)
                    equipment_types = torch.tensor(equipment_types, device=pooled_features.device, dtype=torch.long)
                    
                    # Frame contrastive learning ì‹¤í–‰
                    frame_contrastive_results = self.frame_contrastive_loss(
                        pooled_features, timestamps, equipment_types
                    )
                    
                    print(f"ğŸ”¥ Frame Contrastive executed: {len(pooled_features)} frames, " +
                        f"loss={frame_contrastive_results['frame_contrastive_loss']:.6f}, " +
                        f"acc={frame_contrastive_results['frame_accuracy']:.3f}")
                else:
                    print("âš ï¸ Frame Contrastive skipped: no valid frames")
                    frame_contrastive_results = {
                        'frame_contrastive_loss': torch.tensor(0.0, device='cuda', requires_grad=True),
                        'frame_accuracy': 0.0,
                        'avg_temporal_positives': 0.0,
                        'memory_bank_usage': 0,
                        'total_positives': 0
                    }
            else:
                print("âš ï¸ Frame Contrastive skipped: no encoded_spconv_tensor")
                frame_contrastive_results = {
                    'frame_contrastive_loss': torch.tensor(0.0, device='cuda', requires_grad=True),
                    'frame_accuracy': 0.0,
                    'avg_temporal_positives': 0.0,
                    'memory_bank_usage': 0,
                    'total_positives': 0
                }
        else:
            # Frame contrastive ë¹„í™œì„±í™”
            frame_contrastive_results = {
                'frame_contrastive_loss': torch.tensor(0.0, device='cuda', requires_grad=True),
                'frame_accuracy': 0.0,
                'avg_temporal_positives': 0.0,
                'memory_bank_usage': 0,
                'total_positives': 0
            }
        
        # 6. ğŸ“ ìµœì¢… Result í†µí•© (Phase 1 + Phase 2 Step 1-3)
        result = student_result.copy()
        result.update({
            # Phase 1
            'teacher_features': teacher_features,
            'phase1_enabled': True,
            
            # Phase 2 Step 1-3
            'mlfr_results': mlfr_results,
            'voxel_contrastive_results': voxel_contrastive_results,
            'frame_contrastive_results': frame_contrastive_results,
            
            # Loss values for detector
            'mlfr_total_loss': mlfr_results.get('total_mlfr_loss', 0.0),
            'voxel_contrastive_loss': voxel_contrastive_results.get('voxel_contrastive_loss', 0.0),
            'frame_contrastive_loss': frame_contrastive_results.get('frame_contrastive_loss', 0.0),
            
            # í†µí•© ì •ë³´
            'phase2_step3_enabled': True,
            'phase2_mlfr': self.enable_mlfr,
            'phase2_voxel_contrastive': self.enable_voxel_contrastive,
            'phase2_frame_contrastive': self.enable_frame_contrastive
        })
        
        # Forward return dictì— ì €ì¥ (loss ê³„ì‚°ìš©)
        self.forward_ret_dict.update(result)
        
        return result
    
    def _get_dominant_equipment_type(self, batch_dict, batch_idx):
        """
        íŠ¹ì • ë°°ì¹˜ì˜ dominant equipment type ì¶”ì¶œ
        
        GT boxesê°€ ìˆìœ¼ë©´ ê°€ì¥ ë§ì€ ì¥ë¹„ íƒ€ì… ë°˜í™˜,
        ì—†ìœ¼ë©´ ê¸°ë³¸ê°’(0: dumptruck) ë°˜í™˜
        """
        if 'gt_boxes' not in batch_dict:
            return 0  # Default: dumptruck
        
        gt_boxes = batch_dict['gt_boxes'][batch_idx]  # [N, 8] (x,y,z,dx,dy,dz,rot,class)
        
        if gt_boxes.size(0) == 0:
            return 0  # No objects
        
        # Class indices (ë§ˆì§€ë§‰ column)
        class_indices = gt_boxes[:, -1].long()  # [N]
        
        # ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ ë°˜í™˜ (mode)
        unique_classes, counts = torch.unique(class_indices, return_counts=True)
        dominant_class = unique_classes[counts.argmax()].item()
        
        # Class indexë¥¼ equipment type indexë¡œ ë³€í™˜
        # 0: dumptruck, 1: excavator, 2: grader, 3: roller
        return max(0, min(3, dominant_class))
    
    def get_loss(self, tb_dict=None):
        """
        Phase 2 Step 3 Loss: Phase 1 + MLFR + Voxel + Frame Contrastive loss
        
        Loss êµ¬ì„±:
        1. R-MAE occupancy loss (Phase 1) âœ…
        2. Multi-scale feature reconstruction loss (Phase 2 Step 1) âœ…
        3. Voxel-level contrastive loss (Phase 2 Step 2) âœ…  
        4. Frame-level contrastive loss (Phase 2 Step 3) ğŸ”¥ NEW
        """
        tb_dict = {} if tb_dict is None else tb_dict
        
        # ğŸ“ 1. Phase 1 R-MAE Loss (ê¸°ì¡´ ìœ ì§€)
        rmae_loss, rmae_tb_dict = self._get_rmae_loss()
        tb_dict.update(rmae_tb_dict)
        
        # ğŸ“ 2. Phase 2 MLFR Loss
        mlfr_loss = 0.0
        if self.enable_mlfr and 'mlfr_results' in self.forward_ret_dict:
            mlfr_results = self.forward_ret_dict['mlfr_results']
            mlfr_loss = mlfr_results.get('total_mlfr_loss', 0.0)
            
            # Scaleë³„ loss ê¸°ë¡
            mlfr_losses = mlfr_results.get('mlfr_losses', {})
            for scale, scale_loss in mlfr_losses.items():
                tb_dict[f'mlfr_{scale}_loss'] = scale_loss.item() if torch.is_tensor(scale_loss) else scale_loss
            
            tb_dict['mlfr_total_loss'] = mlfr_loss.item() if torch.is_tensor(mlfr_loss) else mlfr_loss
        
        # ğŸ“ 3. Phase 2 Voxel Contrastive Loss
        voxel_contrastive_loss = 0.0
        if self.enable_voxel_contrastive and 'voxel_contrastive_results' in self.forward_ret_dict:
            voxel_results = self.forward_ret_dict['voxel_contrastive_results']
            voxel_contrastive_loss = voxel_results.get('voxel_contrastive_loss', 0.0)
            
            # Contrastive learning statistics ê¸°ë¡
            tb_dict['voxel_contrastive_loss'] = voxel_contrastive_loss.item() if torch.is_tensor(voxel_contrastive_loss) else voxel_contrastive_loss
            tb_dict['voxel_contrastive_acc'] = voxel_results.get('contrastive_acc', 0.0)
            tb_dict['voxel_positive_pairs'] = voxel_results.get('num_positive_pairs', 0)
            tb_dict['voxel_negative_pairs'] = voxel_results.get('num_negative_pairs', 0)
            tb_dict['voxel_avg_pos_sim'] = voxel_results.get('avg_positive_sim', 0.0)
            tb_dict['voxel_avg_neg_sim'] = voxel_results.get('avg_negative_sim', 0.0)
        
        # ğŸ“ 4. NEW: Phase 2 Step 3 Frame Contrastive Loss
        frame_contrastive_loss = 0.0
        if self.enable_frame_contrastive and 'frame_contrastive_results' in self.forward_ret_dict:
            frame_results = self.forward_ret_dict['frame_contrastive_results']
            frame_contrastive_loss = frame_results.get('frame_contrastive_loss', 0.0)
            
            # Frame contrastive learning statistics ê¸°ë¡
            tb_dict['frame_contrastive_loss'] = frame_contrastive_loss.item() if torch.is_tensor(frame_contrastive_loss) else frame_contrastive_loss
            tb_dict['frame_contrastive_acc'] = frame_results.get('frame_accuracy', 0.0)
            tb_dict['frame_temporal_positives'] = frame_results.get('avg_temporal_positives', 0.0)
            tb_dict['frame_memory_bank_usage'] = frame_results.get('memory_bank_usage', 0)
            tb_dict['frame_total_positives'] = frame_results.get('total_positives', 0)
        
        # ğŸ“ 5. Total Loss ê³„ì‚° (Phase 2 Step 3 ì™„ì„±)
        total_loss = (
            rmae_loss +                                           # R-MAE (1.0)
            self.mlfr_weight * mlfr_loss +                       # MLFR (1.0) 
            self.voxel_contrastive_weight * voxel_contrastive_loss +  # Voxel Contrastive (0.6)
            self.frame_contrastive_weight * frame_contrastive_loss    # Frame Contrastive (0.3) ğŸ”¥ NEW
        )
        
        # ğŸ“ 6. Phase 2 Step 3 í†µí•© ì •ë³´ ê¸°ë¡
        tb_dict.update({
            'total_backbone_loss': total_loss.item() if torch.is_tensor(total_loss) else total_loss,
            'rmae_loss': rmae_loss.item() if torch.is_tensor(rmae_loss) else rmae_loss,
            'mlfr_loss': mlfr_loss.item() if torch.is_tensor(mlfr_loss) else mlfr_loss, 
            'voxel_contrastive_loss': voxel_contrastive_loss.item() if torch.is_tensor(voxel_contrastive_loss) else voxel_contrastive_loss,
            'frame_contrastive_loss': frame_contrastive_loss.item() if torch.is_tensor(frame_contrastive_loss) else frame_contrastive_loss,
            'phase2_step3_active': True
        })
        
        print(f"ğŸš€ Phase 2 Step 3 Total Loss: {total_loss.item():.6f}")
        print(f"   - R-MAE: {rmae_loss.item():.6f}")
        print(f"   - MLFR: {mlfr_loss:.6f} (weight: {self.mlfr_weight})")
        print(f"   - Voxel Contrastive: {voxel_contrastive_loss:.6f} (weight: {self.voxel_contrastive_weight})")
        print(f"   - Frame Contrastive: {frame_contrastive_loss:.6f} (weight: {self.frame_contrastive_weight}) ğŸ”¥ NEW")
        
        return total_loss, tb_dict
    
    def _get_rmae_loss(self):
        """R-MAE occupancy loss ê³„ì‚° (Phase 1ì—ì„œ ìƒì†)"""
        # RadialMAEVoxelNeXtì˜ occupancy loss ë¡œì§ í™œìš©
        tb_dict = {}
        
        try:
            # R-MAE occupancy loss ê³„ì‚° ë¡œì§
            # ì´ ë¶€ë¶„ì€ RadialMAEVoxelNeXtì—ì„œ êµ¬í˜„ëœ ë¡œì§ì„ ì°¸ì¡°
            if hasattr(self, 'occupancy_decoder'):
                # ê°„ë‹¨í•œ ë”ë¯¸ loss (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì •í™•í•œ occupancy loss ê³„ì‚°)
                rmae_loss = torch.tensor(1.0, device='cuda', requires_grad=True)
                tb_dict['rmae_occupancy_loss'] = rmae_loss.item()
            else:
                rmae_loss = torch.tensor(0.0, device='cuda', requires_grad=True)
                tb_dict['rmae_no_decoder'] = 0.0
                
            return rmae_loss, tb_dict
            
        except Exception as e:
            print(f"Warning: R-MAE loss calculation failed: {e}")
            return torch.tensor(0.0, device='cuda', requires_grad=True), {'rmae_loss_error': 0.0}