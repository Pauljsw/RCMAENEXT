# pcdet/models/backbones_3d/components/voxel_contrastive_loss.py
"""
CMAE-3D Phase 2: Voxel-level Contrastive Learning

CMAE-3D ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´:
- Teacher (complete view)ì™€ Student (masked view)ì˜ voxel features ê°„ contrastive learning
- ê°™ì€ ê³µê°„ ìœ„ì¹˜ì˜ voxel = positive pair
- ë‹¤ë¥¸ ê³µê°„ ìœ„ì¹˜ì˜ voxel = negative pair
- InfoNCE lossë¡œ spatial consistency í•™ìŠµ

ì´ ëª¨ë“ˆì€ voxel-levelì—ì„œ teacher-student features ê°„ì˜ 
contrastive learningì„ ìˆ˜í–‰í•˜ì—¬ robust representationì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class VoxelContrastiveLoss(nn.Module):
    """
    ğŸ”¥ CMAE-3D Voxel-level Contrastive Learning
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    1. Teacher features (complete view): ì™„ì „í•œ ê³µê°„ ì •ë³´ í¬í•¨
    2. Student features (masked view): ë¶€ë¶„ì  ê³µê°„ ì •ë³´ í¬í•¨  
    3. ê°™ì€ ì¢Œí‘œ voxel = positive pair (ê³µê°„ì  ì¼ê´€ì„±)
    4. ë‹¤ë¥¸ ì¢Œí‘œ voxel = negative pair (ê³µê°„ì  êµ¬ë¶„ì„±)
    5. InfoNCE lossë¡œ contrastive learning ìˆ˜í–‰
    """
    
    def __init__(self, model_cfg):
        super().__init__()
        
        # Contrastive learning ì„¤ì •
        self.temperature = model_cfg.get('CONTRASTIVE_TEMPERATURE', 0.07)  # CMAE-3D ë…¼ë¬¸ ê¸°ë³¸ê°’
        self.feature_dim = model_cfg.get('FEATURE_DIM', 128)
        self.projection_dim = model_cfg.get('PROJECTION_DIM', 128)
        
        # Positive pair ì°¾ê¸° ì„¤ì •
        self.coord_tolerance = model_cfg.get('COORD_TOLERANCE', 0)  # ì •í™•í•œ ì¢Œí‘œ ë§¤ì¹­ (0) vs ê·¼ì‚¬ ë§¤ì¹­ (>0)
        self.max_negative_samples = model_cfg.get('MAX_NEGATIVE_SAMPLES', 4096)  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        
        # Hard negative mining ì„¤ì • 
        self.enable_hard_negative_mining = model_cfg.get('ENABLE_HARD_NEGATIVE_MINING', True)
        self.hard_negative_ratio = model_cfg.get('HARD_NEGATIVE_RATIO', 0.3)  # 30% hard negatives
        
        # ğŸ“ Feature Projection Heads (Teacher/Student features â†’ contrastive space)
        self.teacher_projector = self._build_projection_head(self.feature_dim, self.projection_dim)
        self.student_projector = self._build_projection_head(self.feature_dim, self.projection_dim)
        
        print(f"ğŸ”¥ Voxel Contrastive Loss initialized:")
        print(f"   - Temperature: {self.temperature}")
        print(f"   - Feature dim: {self.feature_dim} â†’ Projection dim: {self.projection_dim}")
        print(f"   - Hard negative mining: {self.enable_hard_negative_mining}")
        print(f"   - Max negative samples: {self.max_negative_samples}")
    
    def _build_projection_head(self, input_dim, output_dim):
        """Contrastive learningì„ ìœ„í•œ projection head"""
        return nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.ReLU(inplace=True),
            nn.Linear(input_dim, output_dim),
            nn.BatchNorm1d(output_dim)  # ì¤‘ìš”: contrastive learningì—ì„œ BNì´ ì„±ëŠ¥ í–¥ìƒ
        )
    
    def forward(self, teacher_features, student_features, teacher_coords, student_coords):
        """
        Voxel-level contrastive learning forward pass
        
        Args:
            teacher_features: [N_t, feature_dim] Teacher voxel features
            student_features: [N_s, feature_dim] Student voxel features  
            teacher_coords: [N_t, 4] Teacher voxel coordinates (batch, z, y, x)
            student_coords: [N_s, 4] Student voxel coordinates (batch, z, y, x)
        
        Returns:
            contrastive_results: Dict containing contrastive loss and statistics
        """
        # ğŸ“ 1. Feature Projection (Teacher/Student â†’ Contrastive space)
        teacher_proj = self.teacher_projector(teacher_features)  # [N_t, projection_dim]
        student_proj = self.student_projector(student_features)  # [N_s, projection_dim]
        
        # L2 normalization (contrastive learningì—ì„œ ì¤‘ìš”)
        teacher_proj = F.normalize(teacher_proj, dim=1)  # [N_t, projection_dim]
        student_proj = F.normalize(student_proj, dim=1)  # [N_s, projection_dim]
        
        # ğŸ“ 2. Positive Pairs ì°¾ê¸° (ê°™ì€ ì¢Œí‘œ voxel)
        positive_pairs, positive_indices = self._find_positive_pairs(
            teacher_coords, student_coords
        )
        
        if len(positive_pairs) == 0:
            # Positive pairê°€ ì—†ìœ¼ë©´ contrastive learning ë¶ˆê°€
            print("âš ï¸ No positive pairs found for voxel contrastive learning")
            return {
                'voxel_contrastive_loss': torch.tensor(0.0, device=teacher_features.device, requires_grad=True),
                'num_positive_pairs': 0,
                'num_negative_pairs': 0,
                'contrastive_acc': 0.0
            }
        
        # ğŸ“ 3. InfoNCE Loss ê³„ì‚°
        contrastive_loss, stats = self._compute_infonce_loss(
            teacher_proj, student_proj, positive_pairs, positive_indices
        )
        
        return {
            'voxel_contrastive_loss': contrastive_loss,
            'num_positive_pairs': len(positive_pairs),
            'num_negative_pairs': stats['num_negatives'],
            'contrastive_acc': stats['accuracy'],
            'avg_positive_sim': stats['avg_positive_sim'],
            'avg_negative_sim': stats['avg_negative_sim']
        }
    
    def _find_positive_pairs(self, teacher_coords, student_coords):
        """
        ê°™ì€ ê³µê°„ ì¢Œí‘œë¥¼ ê°€ì§„ teacher-student voxel pairs ì°¾ê¸°
        
        Returns:
            positive_pairs: List of (teacher_idx, student_idx) tuples
            positive_indices: Dict for efficient lookup
        """
        # ì¢Œí‘œë¥¼ string keyë¡œ ë³€í™˜ (batch, z, y, x)
        teacher_keys = {}
        student_keys = {}
        
        for idx, coord in enumerate(teacher_coords):
            key = f"{coord[0].item()}_{coord[1].item()}_{coord[2].item()}_{coord[3].item()}"
            teacher_keys[key] = idx
        
        for idx, coord in enumerate(student_coords):
            key = f"{coord[0].item()}_{coord[1].item()}_{coord[2].item()}_{coord[3].item()}"
            if key not in student_keys:
                student_keys[key] = []
            student_keys[key].append(idx)
        
        # ê³µí†µ ì¢Œí‘œ ì°¾ê¸° (positive pairs)
        positive_pairs = []
        positive_indices = {'teacher': [], 'student': []}
        
        for key in teacher_keys:
            if key in student_keys:
                teacher_idx = teacher_keys[key]
                # ê°™ì€ ì¢Œí‘œì— ì—¬ëŸ¬ student voxelì´ ìˆì„ ìˆ˜ ìˆìŒ (rare case)
                for student_idx in student_keys[key]:
                    positive_pairs.append((teacher_idx, student_idx))
                    positive_indices['teacher'].append(teacher_idx)
                    positive_indices['student'].append(student_idx)
        
        return positive_pairs, positive_indices
    
    def _compute_infonce_loss(self, teacher_proj, student_proj, positive_pairs, positive_indices):
        """
        InfoNCE (Contrastive) Loss ê³„ì‚°
        
        ê° student voxelì— ëŒ€í•´:
        - Positive: ê°™ì€ ì¢Œí‘œì˜ teacher voxel (1ê°œ)  
        - Negatives: ë‹¤ë¥¸ ì¢Œí‘œì˜ ëª¨ë“  teacher voxels
        
        Loss = -log(exp(pos_sim/Ï„) / (exp(pos_sim/Ï„) + Î£exp(neg_sim/Ï„)))
        """
        if len(positive_pairs) == 0:
            return torch.tensor(0.0, device=teacher_proj.device, requires_grad=True), {
                'num_negatives': 0, 'accuracy': 0.0, 'avg_positive_sim': 0.0, 'avg_negative_sim': 0.0
            }
        
        total_loss = 0.0
        num_pairs = len(positive_pairs)
        positive_sims = []
        negative_sims = []
        correct_predictions = 0
        
        # Negative samplingì„ ìœ„í•œ teacher features (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        if teacher_proj.size(0) > self.max_negative_samples:
            neg_indices = torch.randperm(teacher_proj.size(0))[:self.max_negative_samples]
            teacher_neg_pool = teacher_proj[neg_indices]  # [max_neg_samples, projection_dim]
        else:
            teacher_neg_pool = teacher_proj  # [N_t, projection_dim]
        
        # ğŸ“ ê° positive pairì— ëŒ€í•´ InfoNCE loss ê³„ì‚°
        for teacher_idx, student_idx in positive_pairs:
            student_feat = student_proj[student_idx:student_idx+1]  # [1, projection_dim]
            teacher_pos_feat = teacher_proj[teacher_idx:teacher_idx+1]  # [1, projection_dim]
            
            # Positive similarity
            pos_sim = torch.mm(student_feat, teacher_pos_feat.t()) / self.temperature  # [1, 1]
            positive_sims.append(pos_sim.item())
            
            # Negative similarities (student vs all other teachers)
            neg_sim = torch.mm(student_feat, teacher_neg_pool.t()) / self.temperature  # [1, max_neg_samples]
            
            # Hard negative mining (ì„ íƒì )
            if self.enable_hard_negative_mining:
                neg_sim = self._apply_hard_negative_mining(neg_sim, pos_sim)
            
            negative_sims.extend(neg_sim.squeeze().tolist())
            
            # InfoNCE loss = -log(exp(pos) / (exp(pos) + Î£exp(neg)))
            all_sims = torch.cat([pos_sim, neg_sim], dim=1)  # [1, 1+neg_samples]
            log_softmax = F.log_softmax(all_sims, dim=1)
            pair_loss = -log_softmax[0, 0]  # Positive sampleì´ ì²« ë²ˆì§¸ (index 0)
            
            total_loss += pair_loss
            
            # Accuracy ê³„ì‚° (positive sampleì´ ê°€ì¥ ë†’ì€ similarityë¥¼ ê°€ì§€ëŠ”ê°€?)
            if torch.argmax(all_sims, dim=1) == 0:
                correct_predictions += 1
        
        # Average loss
        avg_loss = total_loss / num_pairs
        
        # Statistics
        stats = {
            'num_negatives': len(negative_sims),
            'accuracy': correct_predictions / num_pairs if num_pairs > 0 else 0.0,
            'avg_positive_sim': np.mean(positive_sims) if positive_sims else 0.0,
            'avg_negative_sim': np.mean(negative_sims) if negative_sims else 0.0
        }
        
        return avg_loss, stats
    
    def _apply_hard_negative_mining(self, neg_sim, pos_sim):
        """
        Hard negative mining: ê°€ì¥ ì–´ë ¤ìš´ negative samples ì„ íƒ
        
        ë„ˆë¬´ ì‰¬ìš´ negatives (similarityê°€ ë§¤ìš° ë‚®ìŒ)ëŠ” ì œì™¸í•˜ê³ 
        ì–´ë ¤ìš´ negatives (similarityê°€ ë†’ì§€ë§Œ positiveë³´ë‹¤ëŠ” ë‚®ìŒ)ë¥¼ ì„ íƒ
        """
        # Hard negatives ì„ íƒ: positive similarityì— ê°€ê¹Œìš´ negative samples
        num_hard_negatives = max(1, int(neg_sim.size(1) * self.hard_negative_ratio))
        
        # Top-k negative similarities ì„ íƒ (positiveë³´ë‹¤ëŠ” ë‚®ì•„ì•¼ í•¨)
        hard_neg_indices = torch.topk(neg_sim, k=min(num_hard_negatives, neg_sim.size(1)), dim=1)[1]
        hard_neg_sim = torch.gather(neg_sim, 1, hard_neg_indices)
        
        return hard_neg_sim
    
    def get_contrastive_config(self):
        """Contrastive learning ì„¤ì • ë°˜í™˜ (debuggingìš©)"""
        return {
            'temperature': self.temperature,
            'feature_dim': self.feature_dim,
            'projection_dim': self.projection_dim,
            'hard_negative_mining': self.enable_hard_negative_mining,
            'max_negative_samples': self.max_negative_samples
        }