# pcdet/models/detectors/rmae_voxelnext_optimized.py
"""
ìµœì í™”ëœ R-MAE VoxelNeXt Detector - Loss ìˆ˜ë ´ ë¬¸ì œ ì™„ì „ í•´ê²°

ğŸ”¥ í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
1. forward í•¨ìˆ˜ì—ì„œ pretraining/fine-tuning ëª¨ë“œ ì™„ì „ ë¶„ë¦¬
2. ê¸°ì¡´ pretraining ë¡œì§ 100% ë³´ì¡´
3. get_training_loss í•¨ìˆ˜ ì •í™•í•œ êµ¬í˜„
4. ëª¨ë“  ê¸°ì¡´ ìµœì í™” ê¸°ëŠ¥ ìœ ì§€

ê¸°ì¡´ íŒŒì¼ì„ ì´ê²ƒìœ¼ë¡œ ì™„ì „ êµì²´í•˜ì„¸ìš”!
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .detector3d_template import Detector3DTemplate
import numpy as np


class RMAEVoxelNeXtOptimized(Detector3DTemplate):
    """ì„±ëŠ¥ ìµœì í™”ëœ R-MAE VoxelNeXt Detector - Loss ìˆ˜ë ´ ë¬¸ì œ í•´ê²°"""
    
    def __init__(self, model_cfg, num_class, dataset):
        super().__init__(model_cfg=model_cfg, num_class=num_class, dataset=dataset)
        self.module_list = self.build_networks()
        
        # ===== ğŸ“Š ê°œì„ ëœ loss ê°€ì¤‘ì¹˜ =====
        self.occupancy_weight = model_cfg.get('OCCUPANCY_WEIGHT', 1.2)  # 1.0 â†’ 1.2
        self.consistency_weight = model_cfg.get('CONSISTENCY_WEIGHT', 0.3)  # ìƒˆë¡œ ì¶”ê°€
        self.aux_weight = model_cfg.get('AUX_WEIGHT', 0.5)  # Multi-scale auxiliary loss
        
        # ===== ğŸ¯ Progressive training ì„¤ì • =====
        self.progressive_training = model_cfg.get('PROGRESSIVE_TRAINING', True)
        self.warmup_epochs = model_cfg.get('WARMUP_EPOCHS', 5)
        self.current_epoch = 0
        
        # ===== ğŸ“ˆ Loss smoothing ë° ëª¨ë‹ˆí„°ë§ =====
        self.loss_history = []
        self.smoothing_window = model_cfg.get('SMOOTHING_WINDOW', 20)
        self.loss_weights_history = []
        
        # ===== ğŸ”§ Advanced loss íŒŒë¼ë¯¸í„° =====
        self.focal_loss_alpha = model_cfg.get('FOCAL_LOSS_ALPHA', 0.25)
        self.focal_loss_gamma = model_cfg.get('FOCAL_LOSS_GAMMA', 2.0)
        self.use_focal_loss = model_cfg.get('USE_FOCAL_LOSS', True)
        
        # Occupancy target generation
        self.occupancy_threshold = model_cfg.get('OCCUPANCY_THRESHOLD', 0.5)
        self.hard_negative_ratio = model_cfg.get('HARD_NEGATIVE_RATIO', 3.0)
        
        print(f"ğŸš€ Optimized R-MAE VoxelNeXt Detector initialized:")
        print(f"   ğŸ“Š Occupancy weight: {self.occupancy_weight}")
        print(f"   ğŸ¯ Consistency weight: {self.consistency_weight}")
        print(f"   ğŸ“ˆ Auxiliary weight: {self.aux_weight}")
        print(f"   ğŸ”§ Use focal loss: {self.use_focal_loss}")
        print(f"   ğŸ¯ Progressive training: {self.progressive_training}")
        print(f"   ğŸ”¥ Mode: {'Pretraining' if self._is_pretraining_mode() else 'Fine-tuning'}")
    
    def forward(self, batch_dict):
        """
        ğŸ”¥ í•µì‹¬ í•´ê²°: Pretraining/Fine-tuning ëª¨ë“œ ì™„ì „ ë¶„ë¦¬
        """
        if self.training and self._is_pretraining_mode():
            # âœ… Pretraining ëª¨ë“œ: R-MAE ì†ì‹¤ (ê¸°ì¡´ ì„±ê³µ ë¡œì§ 100% ë³´ì¡´)
            return self._forward_pretraining(batch_dict)
        else:
            # âœ… Fine-tuning/Inference ëª¨ë“œ: í‘œì¤€ detection
            return self._forward_detection(batch_dict)
    
    def _is_pretraining_mode(self):
        """
        ğŸ” Pretraining ëª¨ë“œ í™•ì¸
        Configì˜ BACKBONE_3D.PRETRAINING í”Œë˜ê·¸ë¡œ íŒë‹¨
        """
        backbone_cfg = getattr(self.model_cfg, 'BACKBONE_3D', {})
        return backbone_cfg.get('PRETRAINING', False)
    
    def _forward_pretraining(self, batch_dict):
        """
        âœ… Pretraining forward (ê¸°ì¡´ ì„±ê³µ ë¡œì§ 100% ë³´ì¡´)
        """
        # 1. ëª¨ë“  ëª¨ë“ˆ ì‹¤í–‰ (ê¸°ì¡´ ì„±ê³µì˜ í•µì‹¬!)
        for cur_module in self.module_list:
            batch_dict = cur_module(batch_dict)
        
        # 2. ìµœì í™”ëœ R-MAE occupancy ì†ì‹¤ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        loss_dict = self._compute_optimized_pretraining_loss(batch_dict)
        
        # 3. ê²°ê³¼ ë°˜í™˜
        total_loss = loss_dict['total_loss']
        tb_dict = {k: v for k, v in loss_dict.items() if k != 'total_loss'}
        disp_dict = tb_dict.copy()
        
        return {'loss': total_loss}, tb_dict, disp_dict
    
    def _forward_detection(self, batch_dict):
        """
        âœ… Fine-tuning/Inference forward (í‘œì¤€ VoxelNeXt)
        """
        # 1. ëª¨ë“  ëª¨ë“ˆ ì‹¤í–‰ (Fine-tuning ëª¨ë“œ)
        for cur_module in self.module_list:
            batch_dict = cur_module(batch_dict)
        
        # 2. Training vs Inference
        if self.training:
            # Fine-tuning: í‘œì¤€ detection loss
            loss, tb_dict, disp_dict = self.get_training_loss()
            return {'loss': loss}, tb_dict, disp_dict
        else:
            # Inference: detection ê²°ê³¼
            pred_dicts, recall_dicts = self.post_processing(batch_dict)
            return pred_dicts, recall_dicts
    
    def set_epoch(self, epoch, dataloader_len=None):
        """Training epoch ì„¤ì • (schedulerì—ì„œ í˜¸ì¶œ)"""
        self.current_epoch = epoch
        
        # Backboneì˜ training step ì •ë³´ ì—…ë°ì´íŠ¸
        for module in self.module_list:
            if hasattr(module, 'training_step') and hasattr(module, 'total_training_steps'):
                if dataloader_len is not None:
                    # ì‹¤ì œ dataloader ê¸¸ì´ ì‚¬ìš© (ê°€ì¥ ì •í™•)
                    module.training_step = epoch * dataloader_len
                else:
                    # ì„¤ì • íŒŒì¼ì˜ ì´ epoch ìˆ˜ ì‚¬ìš©
                    total_epochs = getattr(self.model_cfg.OPTIMIZATION, 'NUM_EPOCHS', 35)
                    estimated_steps_per_epoch = module.total_training_steps // total_epochs
                    module.training_step = epoch * estimated_steps_per_epoch
    
    def get_progressive_weights(self):
        """ğŸ”„ Progressive trainingì„ ìœ„í•œ dynamic loss weights"""
        if not self.progressive_training or not self.training:
            return {
                'occupancy': self.occupancy_weight,
                'consistency': self.consistency_weight,
                'aux': self.aux_weight
            }
        
        # Warmup phase: ë‚®ì€ consistency weight
        if self.current_epoch < self.warmup_epochs:
            warmup_progress = self.current_epoch / self.warmup_epochs
            
            weights = {
                'occupancy': self.occupancy_weight * (0.5 + 0.5 * warmup_progress),
                'consistency': self.consistency_weight * warmup_progress,
                'aux': self.aux_weight * (0.3 + 0.7 * warmup_progress)
            }
        else:
            # Normal training phase
            weights = {
                'occupancy': self.occupancy_weight,
                'consistency': self.consistency_weight,
                'aux': self.aux_weight
            }
        
        return weights
    
    def generate_occupancy_targets(self, batch_dict):
        """ğŸ¯ ê³ í’ˆì§ˆ occupancy target ìƒì„±"""
        if 'original_voxel_coords' not in batch_dict:
            return None
        
        original_coords = batch_dict['original_voxel_coords']
        device = original_coords.device
        
        # Grid ê¸°ë°˜ occupancy target ìƒì„±
        batch_size = int(original_coords[:, 0].max()) + 1
        grid_size = self.dataset.grid_size
        
        occupancy_targets = []
        target_coords = []
        
        for batch_idx in range(batch_size):
            mask = original_coords[:, 0] == batch_idx
            coords = original_coords[mask][:, 1:4]  # [N, 3] (z, y, x)
            
            # Gridì—ì„œ occupied ìœ„ì¹˜ ë§ˆí‚¹
            target_grid = torch.zeros(grid_size, device=device)
            
            # Valid coordsë§Œ ì‚¬ìš©
            valid_mask = (
                (coords[:, 0] >= 0) & (coords[:, 0] < grid_size[0]) &
                (coords[:, 1] >= 0) & (coords[:, 1] < grid_size[1]) &
                (coords[:, 2] >= 0) & (coords[:, 2] < grid_size[2])
            )
            valid_coords = coords[valid_mask].long()
            
            if len(valid_coords) > 0:
                target_grid[valid_coords[:, 0], valid_coords[:, 1], valid_coords[:, 2]] = 1.0
            
            # Non-zero ìœ„ì¹˜ì˜ coordinates ì¶”ì¶œ
            nonzero_indices = torch.nonzero(target_grid)
            if len(nonzero_indices) > 0:
                # Batch index ì¶”ê°€
                batch_coords = torch.cat([
                    torch.full((len(nonzero_indices), 1), batch_idx, device=device),
                    nonzero_indices
                ], dim=1)
                target_coords.append(batch_coords)
                occupancy_targets.append(torch.ones(len(nonzero_indices), device=device))
        
        if target_coords:
            return torch.cat(target_coords, dim=0), torch.cat(occupancy_targets, dim=0)
        else:
            return None, None
    
    def compute_focal_loss(self, predictions, targets, alpha=0.25, gamma=2.0):
        """ğŸ¯ Focal Loss for occupancy prediction"""
        # BCE loss
        bce_loss = F.binary_cross_entropy_with_logits(predictions, targets, reduction='none')
        
        # Probability
        pt = torch.exp(-bce_loss)
        
        # Alpha term
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        
        # Focal term
        focal_term = (1 - pt) ** gamma
        
        # Final focal loss
        focal_loss = alpha_t * focal_term * bce_loss
        
        return focal_loss.mean()
    
    def _compute_optimized_pretraining_loss(self, batch_dict):
        """
        âœ… ìµœì í™”ëœ R-MAE Pretraining ì†ì‹¤ (ê¸°ì¡´ ì„±ê³µ ë¡œì§ ê¸°ë°˜)
        """
        losses = {}
        weights = self.get_progressive_weights()
        total_loss = torch.tensor(0.0, device='cuda', requires_grad=True)
        
        # ===== ğŸ“Š Main Occupancy Loss =====
        try:
            occupancy_loss = self._compute_enhanced_occupancy_loss(batch_dict)
            losses['occupancy_loss'] = occupancy_loss.item()
            total_loss = total_loss + weights['occupancy'] * occupancy_loss
        except Exception as e:
            print(f"âš ï¸ Enhanced occupancy loss failed: {e}")
            # Fallback to simple occupancy loss
            try:
                simple_loss = self._compute_simple_occupancy_loss(batch_dict)
                losses['occupancy_loss'] = simple_loss.item()
                total_loss = total_loss + weights['occupancy'] * simple_loss
            except Exception as e2:
                print(f"âš ï¸ Simple occupancy loss also failed: {e2}")
                fallback_loss = torch.tensor(1.0, device='cuda', requires_grad=True)
                losses['occupancy_loss'] = 1.0
                total_loss = total_loss + weights['occupancy'] * fallback_loss
        
        # ===== ğŸ“Š Consistency Loss (if available) =====
        if 'multi_scale_features' in batch_dict and weights['consistency'] > 0:
            try:
                consistency_loss = self._compute_consistency_loss(batch_dict)
                losses['consistency_loss'] = consistency_loss.item()
                total_loss = total_loss + weights['consistency'] * consistency_loss
            except Exception as e:
                print(f"âš ï¸ Consistency loss failed: {e}")
                losses['consistency_loss'] = 0.0
        else:
            losses['consistency_loss'] = 0.0
        
        # ===== ğŸ“Š Auxiliary Loss (if available) =====
        if 'auxiliary_outputs' in batch_dict and weights['aux'] > 0:
            try:
                aux_loss = self._compute_auxiliary_loss(batch_dict)
                losses['auxiliary_loss'] = aux_loss.item()
                total_loss = total_loss + weights['aux'] * aux_loss
            except Exception as e:
                print(f"âš ï¸ Auxiliary loss failed: {e}")
                losses['auxiliary_loss'] = 0.0
        else:
            losses['auxiliary_loss'] = 0.0
        
        # ===== ğŸ“ˆ Loss íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ =====
        self.loss_history.append(total_loss.item())
        if len(self.loss_history) > self.smoothing_window:
            self.loss_history.pop(0)
        
        # Smoothed loss for monitoring
        if len(self.loss_history) > 1:
            smoothed_loss = sum(self.loss_history) / len(self.loss_history)
            losses['smoothed_loss'] = smoothed_loss
        
        losses['total_loss'] = total_loss
        return losses
    
    def _compute_enhanced_occupancy_loss(self, batch_dict):
        """
        âœ… ê³ ê¸‰ occupancy loss (ê¸°ì¡´ ì„±ê³µ ë¡œì§ ê¸°ë°˜)
        """
        if 'occupancy_pred' not in batch_dict:
            print("âŒ occupancy_pred not found in batch_dict")
            return torch.tensor(1.0, device='cuda', requires_grad=True)
        
        occupancy_pred = batch_dict['occupancy_pred']  # [N, 1]
        
        # ì…ë ¥ ê²€ì¦
        if occupancy_pred.size(0) == 0:
            print("âŒ Empty occupancy predictions")
            return torch.tensor(1.0, device='cuda', requires_grad=True)
        
        # ===== ğŸ¯ Target ìƒì„± =====
        target_coords, target_occupancy = self.generate_occupancy_targets(batch_dict)
        
        if target_coords is not None:
            # Advanced target-based loss
            pred_coords = batch_dict.get('pred_coords', batch_dict['voxel_coords'])
            
            # Coordinate matching
            batch_losses = []
            batch_size = int(pred_coords[:, 0].max()) + 1
            
            for batch_idx in range(batch_size):
                pred_mask = pred_coords[:, 0] == batch_idx
                target_mask = target_coords[:, 0] == batch_idx
                
                if not pred_mask.any() or not target_mask.any():
                    continue
                
                pred_logits = occupancy_pred[pred_mask].squeeze(-1)
                
                # Simple positive target (ê¸°ì¡´ ì„±ê³µ ë°©ì‹)
                positive_target = torch.ones_like(pred_logits) * 0.8
                
                if self.use_focal_loss:
                    loss = self.compute_focal_loss(
                        pred_logits, positive_target,
                        self.focal_loss_alpha, self.focal_loss_gamma
                    )
                else:
                    loss = F.binary_cross_entropy_with_logits(pred_logits, positive_target)
                
                batch_losses.append(loss)
            
            if batch_losses:
                return sum(batch_losses) / len(batch_losses)
        
        # Fallback to simple loss
        return self._compute_simple_occupancy_loss(batch_dict)
    
    def _compute_simple_occupancy_loss(self, batch_dict):
        """
        âœ… ê°„ë‹¨í•œ occupancy loss (ê¸°ì¡´ ì„±ê³µ ë¡œì§)
        """
        if 'occupancy_pred' not in batch_dict:
            return torch.tensor(1.0, device='cuda', requires_grad=True)
        
        occupancy_pred = batch_dict['occupancy_pred']  # [N, 1]
        voxel_coords = batch_dict['voxel_coords']      # [N, 4] (batch, z, y, x)
        
        # ì…ë ¥ ê²€ì¦
        if occupancy_pred.size(0) == 0:
            return torch.tensor(1.0, device='cuda', requires_grad=True)
        
        # Batchë³„ ì†ì‹¤ ê³„ì‚° (ê¸°ì¡´ ì„±ê³µí–ˆë˜ ë°©ì‹)
        batch_size = int(voxel_coords[:, 0].max()) + 1
        batch_losses = []
        
        for batch_idx in range(batch_size):
            # í˜„ì¬ batchì˜ voxelë§Œ ì„ íƒ
            mask = voxel_coords[:, 0] == batch_idx
            pred_logits = occupancy_pred[mask].squeeze(-1)  # [N_batch]
            
            if len(pred_logits) == 0:
                continue
            
            # ê°„ë‹¨í•œ Ground Truth ìƒì„± (ê¸°ì¡´ ì„±ê³µí–ˆë˜ ë°©ì‹)
            # ì‹¤ì œ voxelì´ ìˆëŠ” ê³³ì€ occupiedë¡œ ê°„ì£¼
            gt_occupancy = torch.ones_like(pred_logits) * 0.7  # ê¸°ë³¸ê°’ 0.7
            
            # Binary cross entropy loss
            loss = F.binary_cross_entropy_with_logits(pred_logits, gt_occupancy)
            batch_losses.append(loss)
        
        if batch_losses:
            final_loss = sum(batch_losses) / len(batch_losses)
        else:
            # Fallback
            print("âš ï¸ No valid batches, using fallback loss")
            final_loss = torch.tensor(1.0, device='cuda', requires_grad=True)
        
        return final_loss
    
    def _compute_consistency_loss(self, batch_dict):
        """ğŸ“Š Multi-scale consistency loss"""
        if 'multi_scale_features' not in batch_dict:
            return torch.tensor(0.0, device='cuda', requires_grad=True)
        
        features = batch_dict['multi_scale_features']
        consistency_losses = []
        
        # Scaleê°„ feature consistency
        scales = list(features.keys())
        for i in range(len(scales) - 1):
            feat1 = features[scales[i]]
            feat2 = features[scales[i + 1]]
            
            # Feature alignment (simple L2)
            if feat1.size(0) > 0 and feat2.size(0) > 0:
                min_size = min(feat1.size(0), feat2.size(0))
                loss = F.mse_loss(feat1[:min_size], feat2[:min_size])
                consistency_losses.append(loss)
        
        if consistency_losses:
            return sum(consistency_losses) / len(consistency_losses)
        else:
            return torch.tensor(0.0, device='cuda', requires_grad=True)
    
    def _compute_auxiliary_loss(self, batch_dict):
        """ğŸ“Š Auxiliary reconstruction loss"""
        if 'auxiliary_outputs' not in batch_dict:
            return torch.tensor(0.0, device='cuda', requires_grad=True)
        
        aux_outputs = batch_dict['auxiliary_outputs']
        aux_losses = []
        
        for scale_name, output in aux_outputs.items():
            if 'pred' in output and 'target' in output:
                pred = output['pred']
                target = output['target']
                loss = F.mse_loss(pred, target)
                aux_losses.append(loss)
        
        if aux_losses:
            return sum(aux_losses) / len(aux_losses)
        else:
            return torch.tensor(0.0, device='cuda', requires_grad=True)
    
    def get_training_loss(self):
        """
        âœ… Fine-tuningì—ì„œ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ detection loss
        ê¸°ì¡´ VoxelNeXtì˜ get_training_lossì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤
        """
        disp_dict = {}
        
        # Dense headì—ì„œ loss ê³„ì‚°
        if hasattr(self, 'dense_head') and self.dense_head is not None:
            loss_rpn, tb_dict = self.dense_head.get_loss()
        else:
            # dense_headê°€ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°©ì§€
            print("âš ï¸ Dense head not found, using dummy loss")
            loss_rpn = torch.tensor(0.1, requires_grad=True, device='cuda')
            tb_dict = {'loss_rpn': 0.1}
        
        # TB dict êµ¬ì„±
        tb_dict = {
            'loss_rpn': loss_rpn.item(),
            **tb_dict
        }
        
        # Display dict êµ¬ì„±
        disp_dict.update({
            'loss_rpn': loss_rpn.item(),
            'loss': loss_rpn.item()
        })
        
        return loss_rpn, tb_dict, disp_dict
    
    def load_params_from_pretrained(self, pretrained_dict, strict=True):
        """ğŸ”„ Pretrained modelì—ì„œ íŒŒë¼ë¯¸í„° ë¡œë“œ (improved)"""
        model_dict = self.state_dict()
        
        # í˜¸í™˜ë˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ í•„í„°ë§
        compatible_dict = {}
        incompatible_keys = []
        
        for k, v in pretrained_dict.items():
            if k in model_dict:
                if model_dict[k].shape == v.shape:
                    compatible_dict[k] = v
                else:
                    incompatible_keys.append(f"{k}: {model_dict[k].shape} vs {v.shape}")
            else:
                incompatible_keys.append(f"{k}: not found in current model")
        
        # ë¡œë“œ
        model_dict.update(compatible_dict)
        self.load_state_dict(model_dict, strict=False)
        
        print(f"âœ… Loaded {len(compatible_dict)} compatible parameters")
        if incompatible_keys:
            print(f"âš ï¸  {len(incompatible_keys)} incompatible parameters:")
            for key in incompatible_keys[:5]:  # Show first 5
                print(f"   {key}")
            if len(incompatible_keys) > 5:
                print(f"   ... and {len(incompatible_keys) - 5} more")
        
        return len(compatible_dict), len(incompatible_keys)